{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gemma-2-2b-it ëª¨ë¸ íŒŒì¸íŠœë‹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. í•™ìŠµ ë°ì´í„° í™•ì¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Œ ë°ì´í„°ì…‹ ì»¬ëŸ¼ëª…: Index(['original_text', 'summary_text'], dtype='object')\n",
      "ğŸ“Œ ë°ì´í„° ìƒ˜í”Œ:\n",
      "                                        original_text  \\\n",
      "0  ì›ê³ ê°€ ì†Œì†íšŒì‚¬ì˜ ë…¸ë™ì¡°í•©ì—ì„œ ë¶„ê·œê°€ ë°œìƒí•˜ì ë…¸ì¡°í™œë™ì„ êµ¬ì‹¤ë¡œ ì •ìƒì ì¸ ê·¼ë¬´ë¥¼ í•´...   \n",
      "1  ìˆ˜ì¶œì…ì—…ì²´ì¸ ì›ê³ ê°€ ì˜ë¥˜ì œí’ˆì„ ì œì¡°ã†ìˆ˜ì¶œí•¨ì— ìˆì–´ ê°™ì€ ê·¸ë£¹ë‚´ ì¢…í•©ë¬´ì—­ìƒì‚¬ì¸ ì†Œì™¸ ...   \n",
      "2  ê°€ë“±ê¸°ë‹´ë³´ê¶Œìê°€ ì œì†Œì „ í™”í•´ì¡°í•­ì— ë”°ë¼ ìê¸° ëª…ì˜ë¡œ ì†Œìœ ê¶Œì´ì „ì˜ ë³¸ë“±ê¸°ë¥¼ ê²½ë£Œí•œ í›„...   \n",
      "3  ê°€. ë¶€ê°€ê°€ì¹˜ì„¸ë²• ì œ22ì¡° ì œ3í•­ ë‹¨ì„œì— ì œ1í˜¸ì™€ ì œ2í˜¸ê°€ ë™ì‹œì— í•´ë‹¹í•œë‹¤ëŠ” ëœ»ì€ ...   \n",
      "4  ì†Œë“ì„¸ë²• ì œ116ì¡° ì œ1í•­ì˜ ê·œì •ì— ì˜í•˜ë©´ ì •ë¶€ëŠ” ê³¼ì„¸í‘œì¤€í™•ì •ì‹ ê³ ë¥¼ í•˜ì—¬ì•¼ í•  ìì—...   \n",
      "\n",
      "                                        summary_text  \n",
      "0  ì›ê³ ê°€  ì£¼ë™í•˜ì—¬ íšŒì‚¬ì—…ë¬´ëŠ¥ë¥ ì„ ì €í•´í•˜ê³  íšŒì‚¬ì—…ë¬´ìƒì˜ ì§€íœ˜ëª…ë ¹ì— ìœ„ë°˜í•˜ì˜€ë‹¤ë©´ ì´ì— ...  \n",
      "1  ìˆ˜ì¶œì…ì—…ì²´ì¸ ì›ê³ ê°€ ì˜ë¥˜ì œí’ˆì„ ì œì¡°ã†ìˆ˜ì¶œí•¨ì— ìˆì–´ ì†Œì™¸ íšŒì‚¬ì˜ ì§ìˆ˜ì¶œì‹¤ì ì„ ì§€ì›í•˜ê¸°...  \n",
      "2  ê°€ë“±ê¸°ë‹´ë³´ê¶Œìê°€ ì œì†Œì „ í™”í•´ì¡°í•­ì— ì˜í•´ ìê¸° ëª…ì˜ë¡œ ì†Œìœ ê¶Œì´ì „ì˜ ë³¸ë“±ê¸°ë¥¼ ê²½ë£Œí•˜ê³  ...  \n",
      "3  ë¶€ê°€ê°€ì¹˜ì„¸ë²• ì œ22ì¡° ì œ3í•­ ë‹¨ì„œì— ì œ1í˜¸ì™€ ì œ2í˜¸ê°€ ë™ì‹œì— í•´ë‹¹í•œë‹¤ëŠ” ì˜ë¯¸ëŠ” ì œ1...  \n",
      "4  ì†Œë“ì„¸ë²• ì œ116ì¡° ì œ1í•­ì— ë”°ë¥´ë©´ ì •ë¶€ëŠ” ê³¼ì„¸í‘œì¤€í™•ì •ì‹ ê³ ë¥¼ í•´ì•¼ í•  ìì— ëŒ€í•´ ë‹¹...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# âœ… CSV íŒŒì¼ ë¡œë“œ\n",
    "csv_path = \"extracted_documents.csv\"\n",
    "df = pd.read_csv(csv_path, encoding=\"utf-8\")\n",
    "\n",
    "# âœ… ë°ì´í„° í™•ì¸\n",
    "print(\"ğŸ“Œ ë°ì´í„°ì…‹ ì»¬ëŸ¼ëª…:\", df.columns)\n",
    "print(\"ğŸ“Œ ë°ì´í„° ìƒ˜í”Œ:\\n\", df.head())\n",
    "\n",
    "# âœ… ì»¬ëŸ¼ëª… ë³€ê²½ (í•„ìš” ì‹œ)\n",
    "df = df.rename(columns={\"original_text\": \"input_text\", \"summary_text\": \"target_text\"})\n",
    "\n",
    "# âœ… ì €ì¥ (ë³€ê²½ëœ íŒŒì¼ì„ í™œìš©í•  ê²½ìš°)\n",
    "df.to_csv(\"extracted_documents_updated.csv\", index=False, encoding=\"utf-8-sig\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "from main import access_token  # âœ… main.pyì—ì„œ Access Token ê°€ì ¸ì˜¤ê¸°\n",
    "\n",
    "def load_model_and_tokenizer(model_name=\"google/gemma-2-2b-it\", quantized=True):\n",
    "    \"\"\"\n",
    "    Hugging Faceì—ì„œ Gemma ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ë¡œë“œí•˜ëŠ” í•¨ìˆ˜.\n",
    "    - QLoRA ì ìš© (4-bit ì–‘ìí™”)\n",
    "    - main.pyì—ì„œ Access Tokenì„ ê°€ì ¸ì™€ ì¸ì¦\n",
    "    \"\"\"\n",
    "    if not access_token:\n",
    "        raise ValueError(\"âŒ Hugging Face Access Tokenì´ ì—†ìŠµë‹ˆë‹¤. í™˜ê²½ ë³€ìˆ˜ë¥¼ í™•ì¸í•˜ì„¸ìš”.\")\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, token=access_token)\n",
    "\n",
    "    if quantized:\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_compute_dtype=torch.float16,  # âœ… FP16 ì—°ì‚° ì‚¬ìš©\n",
    "            bnb_4bit_quant_type=\"nf4\",  # âœ… QLoRA ìµœì í™”ëœ nf4 ì–‘ìí™” ì ìš©\n",
    "            bnb_4bit_use_double_quant=True  # âœ… ì´ì¤‘ ì–‘ìí™” ì ìš©\n",
    "        )\n",
    "        device_map = {\"\": \"cuda:1\"} if torch.cuda.is_available() else \"cpu\"  # âœ… GPU ì„¤ì •\n",
    "\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            quantization_config=bnb_config,\n",
    "            device_map=device_map,\n",
    "            token=access_token  # âœ… ì¸ì¦ ì¶”ê°€\n",
    "        )\n",
    "    else:\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name, token=access_token)\n",
    "\n",
    "    return model, tokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ë°ì´í„°ì…‹ ë¶ˆëŸ¬ì˜¤ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ì¤€ë¹„ëœ ë°ì´í„°ì…‹ ì˜ˆì‹œ: {'input_text': 'ì›ê³ ê°€ ì†Œì†íšŒì‚¬ì˜ ë…¸ë™ì¡°í•©ì—ì„œ ë¶„ê·œê°€ ë°œìƒí•˜ì ë…¸ì¡°í™œë™ì„ êµ¬ì‹¤ë¡œ ì •ìƒì ì¸ ê·¼ë¬´ë¥¼ í•´íƒœí•˜ê³ , ë…¸ì¡°ì¡°í•©ì¥ì´ ì‚¬ì„í•œ ê²½ìš°, ë…¸ë™ì¡°í•©ê·œì•½ì— ë™ ì¡°í•©ì¥ì˜ ì§ë¬´ë¥¼ ëŒ€í–‰í•  ìë¥¼ ê·œì •í•´ ë‘ê³  ìˆìŒì—ë„ ì›ê³  ìì‹ ì´ ì£¼ë™í•˜ì—¬ ë…¸ì¡°ìì¹˜ìˆ˜ìŠµëŒ€ì±…ìœ„ì›íšŒë¥¼ êµ¬ì„±í•˜ì—¬ ê·¸ ìœ„ì›ì¥ìœ¼ë¡œ í”¼ì„ ë˜ì–´ ê·¼ë¬´ì‹œê°„ì¤‘ì—ë„ ë…¸ì¡°í™œë™ì„ ë²Œì—¬ ìš´ìˆ˜ì—…ì²´ì¸ ì†Œì†íšŒì‚¬ì˜ ì—…ë¬´ì— ì§€ì¥ì„ ì´ˆë˜í•˜ê³  ì¢…ì—…ì›ë“¤ì—ê²Œë„ ë‚˜ìœ ì˜í–¥ì„ ë¼ì³ ì†Œì†íšŒì‚¬ê°€ ì·¨ì—…ê·œì¹™ì„ ìœ„ë°˜í•˜ê³  ê³ ì˜ë¡œ íšŒì‚¬ì—…ë¬´ëŠ¥ë¥ ì„ ì €í•´í•˜ì˜€ìœ¼ë©° íšŒì‚¬ì—…ë¬´ìƒì˜ ì§€íœ˜ëª…ë ¹ì— ìœ„ë°˜í•˜ì˜€ìŒì„ ì´ìœ ë¡œ ì›ê³ ë¥¼ ì§•ê³„í•´ê³  í•˜ì˜€ë‹¤ë©´, ì´ëŠ” ì›ê³ ì˜ ë…¸ë™ì¡°í•© í™œë™ê³¼ëŠ” ê´€ê³„ì—†ì´ íšŒì‚¬ì·¨ì—…ê·œì¹™ì— ì˜í•˜ì—¬ ì‚¬ë‚´ì§ˆì„œë¥¼ ìœ ì§€í•˜ê¸° ìœ„í•œ ì‚¬ìš©ì ê³ ìœ ì˜ ì§•ê³„ê¶Œì— ê¸°í•˜ì—¬ ì´ë£¨ì–´ì§„ ì •ë‹¹í•œ ì§•ê³„ê¶Œì˜ í–‰ì‚¬ë¡œ ë³´ì•„ì•¼ í•œë‹¤.', 'target_text': 'ì›ê³ ê°€  ì£¼ë™í•˜ì—¬ íšŒì‚¬ì—…ë¬´ëŠ¥ë¥ ì„ ì €í•´í•˜ê³  íšŒì‚¬ì—…ë¬´ìƒì˜ ì§€íœ˜ëª…ë ¹ì— ìœ„ë°˜í•˜ì˜€ë‹¤ë©´ ì´ì— ë”°ë¥¸ ì§•ê³„í•´ê³ ëŠ” ì‚¬ë‚´ì§ˆì„œë¥¼ ìœ ì§€í•˜ê¸° ìœ„í•œ ì‚¬ìš©ì ê³ ìœ ì˜ ì •ë‹¹í•œ ì§•ê³„ê¶Œì˜ í–‰ì‚¬ë¡œ ë³´ì•„ì•¼ í•œë‹¤.'}\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "def prepare_dataset(csv_path, encoding=\"utf-8\"):\n",
    "    \"\"\"\n",
    "    CSV íŒŒì¼ì„ ë¶ˆëŸ¬ì™€ì„œ ë°ì´í„°ì…‹ì„ êµ¬ì¶•í•©ë‹ˆë‹¤.\n",
    "    'input_text'ì™€ 'target_text' ì»¬ëŸ¼ì„ Hugging Face Dataset í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path, encoding=encoding)\n",
    "\n",
    "    # âœ… ì»¬ëŸ¼ëª… í™•ì¸ í›„ ë³€í™˜\n",
    "    if \"original_text\" in df.columns and \"summary_text\" in df.columns:\n",
    "        df = df.rename(columns={\"original_text\": \"input_text\", \"summary_text\": \"target_text\"})\n",
    "    elif \"input_text\" not in df.columns or \"target_text\" not in df.columns:\n",
    "        raise ValueError(\"CSV íŒŒì¼ì— 'input_text' ë˜ëŠ” 'target_text' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤. ì»¬ëŸ¼ëª…ì„ í™•ì¸í•˜ì„¸ìš”!\")\n",
    "\n",
    "    # âœ… Pandas DataFrame â†’ Hugging Face Dataset ë³€í™˜\n",
    "    dataset = Dataset.from_pandas(df)\n",
    "    return dataset\n",
    "\n",
    "# âœ… ì‚¬ìš© ì˜ˆì‹œ\n",
    "csv_path = \"extracted_documents_updated.csv\"\n",
    "raw_dataset = prepare_dataset(csv_path, encoding=\"utf-8\")\n",
    "print(\"âœ… ì¤€ë¹„ëœ ë°ì´í„°ì…‹ ì˜ˆì‹œ:\", raw_dataset[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ë°ì´í„° í† í°í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d5fdbfde98d44008e6e90ad9c4d44c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15f79130c2fa4704863abe538c21e24c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/24329 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… í† í°í™” ì™„ë£Œ! ì²« ë²ˆì§¸ ë°ì´í„° ìƒ˜í”Œ: {'input_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 237399, 236464, 236361, 44997, 238982, 237880, 236417, 236137, 61138, 237358, 237602, 237961, 22803, 70754, 240753, 236361, 205209, 236345, 236645, 61138, 237602, 240090, 237358, 236392, 49061, 238146, 236375, 35467, 237047, 85024, 171351, 237908, 236791, 56787, 239190, 48060, 235269, 61138, 237602, 237602, 237961, 237199, 235832, 17309, 238700, 236511, 95917, 235269, 61138, 237358, 237602, 237961, 240753, 239618, 236179, 49697, 42916, 237961, 237199, 236137, 143636, 237908, 236791, 26801, 238356, 238080, 34103, 236791, 235248, 240753, 236864, 237138, 93828, 236464, 21167, 238036, 209929, 68586, 236464, 127637, 235832, 40712, 237358, 72494, 61138, 237602, 236645, 237924, 236669, 237152, 236800, 240460, 237601, 237399, 237880, 236791, 220641, 72494, 20350, 41423, 237399, 237199, 26291, 99742, 237700, 141048, 171351, 237908, 210450, 237935, 209929, 61138, 237602, 240090, 237358, 236392, 235248, 241588, 237386, 118566, 236669, 238391, 238308, 236589, 44997, 238982, 237880, 236417, 236137, 142995, 237908, 236179, 34805, 237199, 236392, 87634, 238608, 48060, 86126, 238391, 237399, 215895, 236840, 38585, 244050, 52604, 240112, 236392, 235248, 241892, 242391, 44997, 238982, 237880, 236417, 236361, 174375, 238391, 240753, 242930, 236392, 41423, 238559, 48060, 46749, 236137, 236375, 84961, 236417, 238391, 237908, 239474, 243156, 236392, 80404, 237138, 236345, 239867, 101715, 84961, 236417, 238391, 237908, 237047, 236137, 34805, 243500, 238068, 240446, 236179, 41423, 238559, 236345, 239867, 238036, 236392, 11464, 237766, 236375, 68586, 236464, 236791, 235248, 241330, 238002, 237138, 236464, 30181, 129761, 237722, 235269, 11464, 236214, 68586, 236464, 236137, 61138, 237358, 237602, 237961, 147226, 237358, 237233, 236214, 217761, 238739, 235832, 84961, 236417, 240826, 238391, 240753, 242930, 236179, 23594, 72494, 17309, 238151, 239574, 236554, 236791, 46355, 236183, 72159, 168947, 61943, 236645, 46749, 237766, 236137, 235248, 241330, 238002, 239765, 236179, 28693, 72494, 11464, 238949, 236770, 237589, 35467, 238272, 236511, 235248, 241330, 238002, 239765, 236137, 105560, 236417, 236375, 29283, 236655, 238305, 153422, 235265], 'attention_mask': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 237399, 236464, 236361, 139, 237014, 237358, 72494, 84961, 236417, 238391, 237908, 239474, 243156, 236392, 80404, 237138, 48060, 84961, 236417, 238391, 237908, 237047, 236137, 34805, 243500, 238068, 240446, 236179, 41423, 238559, 194900, 237722, 11464, 236179, 103293, 239296, 235248, 241330, 238002, 237138, 236464, 236214, 17309, 238151, 239574, 236554, 236791, 46355, 236183, 72159, 168947, 61943, 236645, 46749, 237766, 236137, 35467, 238272, 236511, 235248, 241330, 238002, 239765, 236137, 105560, 236417, 236375, 29283, 236655, 238305, 153422, 235265]}\n"
     ]
    }
   ],
   "source": [
    "def tokenize_dataset(dataset, tokenizer, max_input_length=1024, max_target_length=256):\n",
    "    \"\"\"\n",
    "    ë°ì´í„°ì…‹ì„ í† í°í™”í•˜ì—¬ ëª¨ë¸ì´ í•™ìŠµí•  ìˆ˜ ìˆëŠ” í˜•íƒœë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    if tokenizer is None:\n",
    "        raise ValueError(\"âŒ tokenizerê°€ ì •ì˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. `load_model_and_tokenizer`ë¥¼ í˜¸ì¶œí•˜ì—¬ tokenizerë¥¼ ë¨¼ì € ë¡œë“œí•˜ì„¸ìš”.\")\n",
    "\n",
    "    def tokenize_function(example):\n",
    "        model_inputs = tokenizer(\n",
    "            example[\"input_text\"],\n",
    "            max_length=max_input_length,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\"\n",
    "        )\n",
    "        labels = tokenizer(\n",
    "            example[\"target_text\"],\n",
    "            max_length=max_target_length,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\"\n",
    "        )\n",
    "        model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "        return model_inputs\n",
    "\n",
    "    tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=dataset.column_names)\n",
    "    return tokenized_dataset\n",
    "\n",
    "# âœ… ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
    "model, tokenizer = load_model_and_tokenizer(\"google/gemma-2-2b-it\")\n",
    "\n",
    "# âœ… ë°ì´í„°ì…‹ ë¡œë“œ\n",
    "csv_path = \"extracted_documents_updated.csv\"\n",
    "raw_dataset = prepare_dataset(csv_path, encoding=\"utf-8\")\n",
    "\n",
    "# âœ… ìˆ˜ì •ëœ ì½”ë“œ ì‚¬ìš©\n",
    "tokenized_dataset = tokenize_dataset(raw_dataset, tokenizer)\n",
    "print(\"âœ… í† í°í™” ì™„ë£Œ! ì²« ë²ˆì§¸ ë°ì´í„° ìƒ˜í”Œ:\", tokenized_dataset[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Fine_Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import TrainingArguments, Trainer, DataCollatorForSeq2Seq\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "def fine_tune_model(model, tokenizer, tokenized_dataset, output_dir=\"./fine_tuned_results\", epochs=3, batch_size=1):\n",
    "    \"\"\"\n",
    "    LoRA ê¸°ë°˜ Fine-Tuning ì‹¤í–‰ í•¨ìˆ˜.\n",
    "    \"\"\"\n",
    "\n",
    "    # âœ… LoRA ì–´ëŒ‘í„° êµ¬ì„±\n",
    "    lora_config = LoraConfig(\n",
    "        r=16,  # LoRA ë­í¬ (ë©”ëª¨ë¦¬ì— ë§ê²Œ ì¡°ì • ê°€ëŠ¥)\n",
    "        lora_alpha=32,  # LoRA í•™ìŠµë¥  ê³„ìˆ˜\n",
    "        lora_dropout=0.05,  # ë“œë¡­ì•„ì›ƒ ì ìš©\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\"  # ìºì£¼ì–¼ ì–¸ì–´ ëª¨ë¸ë§ (Gemma ëª¨ë¸ì— ë§ì¶¤)\n",
    "    )\n",
    "\n",
    "    # âœ… LoRA ì ìš©\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    \n",
    "    # âœ… ë°ì´í„°ì…‹ ë¶„í• : í•™ìŠµ(train) & ê²€ì¦(eval)\n",
    "    split_dataset = tokenized_dataset.train_test_split(test_size=0.1, seed=42)\n",
    "    train_dataset = split_dataset[\"train\"]\n",
    "    eval_dataset = split_dataset[\"test\"]\n",
    "    \n",
    "    # âœ… ë°ì´í„° ì½œë ˆì´í„°: íŒ¨ë”© ìë™ ì²˜ë¦¬\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "    # âœ… í•™ìŠµ íŒŒë¼ë¯¸í„° ì„¤ì •\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        learning_rate=3e-4,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        num_train_epochs=epochs,\n",
    "        weight_decay=0.01,\n",
    "        fp16=True,  # 16-bit ì—°ì‚° ì‚¬ìš©\n",
    "        save_total_limit=2,\n",
    "        logging_steps=50,\n",
    "        seed=42\n",
    "    )\n",
    "\n",
    "    # âœ… Trainer ê°ì²´ ìƒì„±\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator\n",
    "    )\n",
    "\n",
    "    # âœ… Fine-Tuning ì‹¤í–‰\n",
    "    trainer.train()\n",
    "\n",
    "    # âœ… í•™ìŠµ ì™„ë£Œëœ ëª¨ë¸ ì €ì¥\n",
    "    trainer.save_model(output_dir)\n",
    "\n",
    "# âœ… ì‚¬ìš© ì˜ˆì‹œ\n",
    "# fine_tune_model(model, tokenizer, tokenized_dataset, output_dir=\"./fine_tuned_results\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ë©”ì¸ í•¨ìˆ˜ ì‹¤í–‰ ì½”ë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d78f5a403cd453296eebbedc246a583",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "babc84d55c97453f9b6654ec85941e00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/24329 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wanted-1/miniconda3/envs/dl_project/lib/python3.9/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/tmp/ipykernel_2320502/28924360.py:46: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-03-02 09:41:39,923] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/wanted-1/miniconda3/envs/dl_project/compiler_compat/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/wanted-1/miniconda3/envs/dl_project/compiler_compat/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "You can't train a model that has been loaded in 8-bit or 4-bit precision on a different device than the one you're training on. Make sure you loaded the model on the correct device using for example `device_map={'':torch.cuda.current_device()}` or `device_map={'':torch.xpu.current_device()}`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m     fine_tune_model(model, tokenizer, tokenized_dataset, output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./fine_tuned_results\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 24\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 21\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m tokenized_dataset \u001b[38;5;241m=\u001b[39m tokenize_dataset(raw_dataset, tokenizer)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# âœ… Fine-Tuning ì‹¤í–‰\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m \u001b[43mfine_tune_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenized_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./fine_tuned_results\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 56\u001b[0m, in \u001b[0;36mfine_tune_model\u001b[0;34m(model, tokenizer, tokenized_dataset, output_dir, epochs, batch_size)\u001b[0m\n\u001b[1;32m     46\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     47\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     48\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     52\u001b[0m     data_collator\u001b[38;5;241m=\u001b[39mdata_collator\n\u001b[1;32m     53\u001b[0m )\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# âœ… Fine-Tuning ì‹¤í–‰\u001b[39;00m\n\u001b[0;32m---> 56\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# âœ… í•™ìŠµ ì™„ë£Œëœ ëª¨ë¸ ì €ì¥\u001b[39;00m\n\u001b[1;32m     59\u001b[0m trainer\u001b[38;5;241m.\u001b[39msave_model(output_dir)\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_project/lib/python3.9/site-packages/transformers/trainer.py:2241\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2239\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2240\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2242\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2246\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_project/lib/python3.9/site-packages/transformers/trainer.py:2365\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2363\u001b[0m         model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mprepare(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[1;32m   2364\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2365\u001b[0m         model, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2366\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2367\u001b[0m     \u001b[38;5;66;03m# to handle cases wherein we pass \"DummyScheduler\" such as when it is specified in DeepSpeed config.\u001b[39;00m\n\u001b[1;32m   2368\u001b[0m     model, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr_scheduler \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mprepare(\n\u001b[1;32m   2369\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr_scheduler\n\u001b[1;32m   2370\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_project/lib/python3.9/site-packages/accelerate/accelerator.py:1389\u001b[0m, in \u001b[0;36mAccelerator.prepare\u001b[0;34m(self, device_placement, *args)\u001b[0m\n\u001b[1;32m   1387\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp8_backend \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMSAMP\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1388\u001b[0m         args, device_placement \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_msamp(\u001b[38;5;241m*\u001b[39margs, device_placement\u001b[38;5;241m=\u001b[39mdevice_placement)\n\u001b[0;32m-> 1389\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1390\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfirst_pass\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_placement\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43md\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_placement\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1391\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1392\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_one(obj, device_placement\u001b[38;5;241m=\u001b[39md) \u001b[38;5;28;01mfor\u001b[39;00m obj, d \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(result, device_placement))\n\u001b[1;32m   1393\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tpu_should_fix_optimizer:\n\u001b[1;32m   1394\u001b[0m     \u001b[38;5;66;03m# 2. grabbing new model parameters\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_project/lib/python3.9/site-packages/accelerate/accelerator.py:1390\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1387\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp8_backend \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMSAMP\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1388\u001b[0m         args, device_placement \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_msamp(\u001b[38;5;241m*\u001b[39margs, device_placement\u001b[38;5;241m=\u001b[39mdevice_placement)\n\u001b[1;32m   1389\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\n\u001b[0;32m-> 1390\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfirst_pass\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_placement\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43md\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m obj, d \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(args, device_placement)\n\u001b[1;32m   1391\u001b[0m     )\n\u001b[1;32m   1392\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_one(obj, device_placement\u001b[38;5;241m=\u001b[39md) \u001b[38;5;28;01mfor\u001b[39;00m obj, d \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(result, device_placement))\n\u001b[1;32m   1393\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tpu_should_fix_optimizer:\n\u001b[1;32m   1394\u001b[0m     \u001b[38;5;66;03m# 2. grabbing new model parameters\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_project/lib/python3.9/site-packages/accelerate/accelerator.py:1263\u001b[0m, in \u001b[0;36mAccelerator._prepare_one\u001b[0;34m(self, obj, first_pass, device_placement)\u001b[0m\n\u001b[1;32m   1261\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_data_loader(obj, device_placement\u001b[38;5;241m=\u001b[39mdevice_placement)\n\u001b[1;32m   1262\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule):\n\u001b[0;32m-> 1263\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_placement\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_placement\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1264\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mOptimizer):\n\u001b[1;32m   1265\u001b[0m     optimizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_optimizer(obj, device_placement\u001b[38;5;241m=\u001b[39mdevice_placement)\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_project/lib/python3.9/site-packages/accelerate/accelerator.py:1491\u001b[0m, in \u001b[0;36mAccelerator.prepare_model\u001b[0;34m(self, model, device_placement, evaluation_mode)\u001b[0m\n\u001b[1;32m   1488\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mdevice(current_device_index) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice:\n\u001b[1;32m   1489\u001b[0m         \u001b[38;5;66;03m# if on the first device (GPU 0) we don't care\u001b[39;00m\n\u001b[1;32m   1490\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mindex \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m (current_device_index \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m-> 1491\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1492\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt train a model that has been loaded in 8-bit or 4-bit precision on a different device than the one \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1493\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mre training on. Make sure you loaded the model on the correct device using for example `device_map=\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:torch.cuda.current_device()}` or `device_map=\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:torch.xpu.current_device()}`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1494\u001b[0m             )\n\u001b[1;32m   1495\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1496\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m model_devices \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_bitsandbytes_multi_backend_available())\n\u001b[1;32m   1497\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m model_devices \u001b[38;5;129;01mand\u001b[39;00m is_xpu_available())\n\u001b[1;32m   1498\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisk\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m model_devices\n\u001b[1;32m   1499\u001b[0m ):\n\u001b[1;32m   1500\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1501\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt train a model that has been loaded in 8-bit or 4-bit precision with CPU or disk offload. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1502\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf you want train the 8-bit or 4-bit model in CPU, please install bitsandbytes with multi-backend, see https://huggingface.co/docs/bitsandbytes/main/en/installation#multi-backend\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1503\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: You can't train a model that has been loaded in 8-bit or 4-bit precision on a different device than the one you're training on. Make sure you loaded the model on the correct device using for example `device_map={'':torch.cuda.current_device()}` or `device_map={'':torch.xpu.current_device()}`"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "def main():\n",
    "    # âœ… í™˜ê²½ ë³€ìˆ˜ ì„¤ì • (ë©”ëª¨ë¦¬ ë‹¨í¸í™” ì™„í™”)\n",
    "    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "    # âœ… GPU ì„¤ì •\n",
    "    device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Using device:\", device)\n",
    "\n",
    "    # âœ… ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
    "    model, tokenizer = load_model_and_tokenizer(\"google/gemma-2-2b-it\", quantized=True)\n",
    "\n",
    "    # âœ… ë°ì´í„° ë¡œë“œ ë° í† í°í™”\n",
    "    csv_path = \"extracted_documents_updated.csv\"\n",
    "    raw_dataset = prepare_dataset(csv_path, encoding=\"utf-8\")\n",
    "    tokenized_dataset = tokenize_dataset(raw_dataset, tokenizer)\n",
    "\n",
    "    # âœ… Fine-Tuning ì‹¤í–‰\n",
    "    fine_tune_model(model, tokenizer, tokenized_dataset, output_dir=\"./fine_tuned_results\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
