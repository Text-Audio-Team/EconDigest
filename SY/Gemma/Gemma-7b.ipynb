{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gemma-7b 활용/ 요약 모델 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "#import os\n",
    "# # 서버의 GPU 1번 사용하게 만들기 \n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # 1번 GPU만 노출됨\n",
    "import torch\n",
    "\n",
    "# 원하는 GPU를 지정 (예: 두 번째 GPU 사용)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 데이터셋 구축"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "012ada7c0222471c8b50d1de99cb591d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['name', 'delivery_date', 'documents'],\n",
      "        num_rows: 24329\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import zipfile\n",
    "from datasets import load_dataset\n",
    "\n",
    "def load_and_build_dataset_from_zip(zip_path, extracted_dir):\n",
    "    \"\"\"\n",
    "    zip 파일 내의 JSON 파일들을 동적으로 검색하여 데이터셋을 구성합니다.\n",
    "    \n",
    "    1. 지정된 경로에 압축 파일을 해제합니다.\n",
    "    2. glob 모듈을 사용해 모든 JSON 파일을 검색합니다.\n",
    "    3. 정규표현식을 활용하여 파일명에 'train' (대소문자 구분 없이)이 포함된 파일만 필터링합니다.\n",
    "    4. 필터링된 파일들을 이용해 Hugging Face의 load_dataset 함수로 데이터셋을 구성합니다.\n",
    "    \n",
    "    :param zip_path: 압축 파일의 경로\n",
    "    :param extracted_dir: 압축 해제할 디렉토리\n",
    "    :return: 구성된 데이터셋\n",
    "    \"\"\"\n",
    "    # 압축 해제할 디렉토리가 없으면 생성합니다.\n",
    "    if not os.path.exists(extracted_dir):\n",
    "        os.makedirs(extracted_dir)\n",
    "    \n",
    "    # zip 파일을 열어 지정된 디렉토리에 압축 해제\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extracted_dir)\n",
    "    \n",
    "    # glob 모듈을 사용하여 extracted_dir 내의 모든 JSON 파일 검색\n",
    "    json_files = glob.glob(os.path.join(extracted_dir, \"*.json\"))\n",
    "    \n",
    "    # 정규표현식을 사용하여 파일명에 'train'이 포함된 파일만 필터링 (대소문자 무시)\n",
    "    train_files = [f for f in json_files if re.search(r\"train\", os.path.basename(f), re.IGNORECASE)]\n",
    "    \n",
    "    if not train_files:\n",
    "        raise ValueError(\"압축 해제된 디렉토리에서 'train'이 포함된 JSON 파일을 찾을 수 없습니다.\")\n",
    "    \n",
    "    # validation 파일도 필요하면 비슷한 방식으로 필터링 가능 (예시)\n",
    "    validation_files = [f for f in json_files if re.search(r\"validation\", os.path.basename(f), re.IGNORECASE)]\n",
    "    \n",
    "    # 데이터 파일 딕셔너리 생성: load_dataset은 리스트로 전달받은 여러 파일들을 자동으로 병합합니다.\n",
    "    data_files = {\"train\": train_files}\n",
    "    if validation_files:\n",
    "        data_files[\"validation\"] = validation_files\n",
    "    \n",
    "    # JSON 파일들로부터 데이터셋 구성\n",
    "    dataset = load_dataset(\"json\", data_files=data_files)\n",
    "    return dataset\n",
    "\n",
    "# 사용 예시:\n",
    "zip_path = \"/home/wanted-1/potenup-workspace/Project/project3/team2/학습데이터/06_문서요약 텍스트/Training/법률_train_original.zip\"\n",
    "extracted_dir = \"/home/wanted-1/potenup-workspace/Project/project3/team2/학습데이터/06_문서요약 텍스트/Training/extracted\"\n",
    "\n",
    "dataset = load_and_build_dataset_from_zip(zip_path, extracted_dir)\n",
    "print(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_original_and_summary(document):\n",
    "    \"\"\"\n",
    "    주어진 document 딕셔너리에서 원문과 요약문을 추출합니다.\n",
    "    \n",
    "    원문:\n",
    "        - 'text' 필드는 리스트 안에 리스트 형태로 구성되어 있습니다.\n",
    "        - 각 내부 딕셔너리의 'sentence' 값을 추출하여 하나의 문자열로 결합합니다.\n",
    "    요약문:\n",
    "        - 'abstractive' 필드에 요약문이 문자열 리스트로 제공되며,\n",
    "          첫 번째 요소를 최종 요약문으로 사용합니다.\n",
    "    \"\"\"\n",
    "    # 원문 추출: 'text' 필드의 각 문장을 결합합니다.\n",
    "    original_sentences = []\n",
    "    for block in document.get(\"text\", []):\n",
    "        for sentence_info in block:\n",
    "            sentence = sentence_info.get(\"sentence\", \"\")\n",
    "            if sentence:\n",
    "                original_sentences.append(sentence.strip())\n",
    "    original_text = \" \".join(original_sentences)\n",
    "    \n",
    "    # 요약문 추출: 'abstractive' 필드의 첫 번째 요소 사용\n",
    "    summary_list = document.get(\"abstractive\", [])\n",
    "    summary_text = summary_list[0].strip() if summary_list else \"\"\n",
    "    \n",
    "    return original_text, summary_text\n",
    "\n",
    "# 전체 데이터셋의 모든 문서에서 원문과 요약문을 추출하는 반복문\n",
    "# dataset['train']['documents'] 가 모든 문서를 포함하는 리스트라고 가정합니다.\n",
    "all_originals = []\n",
    "all_summaries = []\n",
    "\n",
    "for document in dataset['train']['documents']:\n",
    "    original_text, summary_text = extract_original_and_summary(document)\n",
    "    all_originals.append(original_text)\n",
    "    all_summaries.append(summary_text)\n",
    "\n",
    "# # 추출된 결과를 문서별로 출력합니다.\n",
    "# for idx, (orig, summ) in enumerate(zip(all_originals, all_summaries)):\n",
    "#     print(f\"Document {idx+1}\")\n",
    "#     print(\"Original Text:\")\n",
    "#     print(orig)\n",
    "#     print(\"Summary:\")\n",
    "#     print(summ)\n",
    "#     print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 예를 들어, 전체 문서에서 추출한 원문과 요약문 리스트가 있다고 가정합니다.\n",
    "data = {\n",
    "    \"original_text\": all_originals,\n",
    "    \"summary_text\": all_summaries\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# DataFrame 내용 확인\n",
    "# print(df.head())\n",
    "\n",
    "# CSV 파일로 저장하기\n",
    "df.to_csv(\"extracted_documents.csv\", index=False, encoding=\"utf-8-sig\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 데이터셋 포맷팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_text': '원고가 소속회사의 노동조합에서 분규가 발생하자 노조활동을 구실로 정상적인 근무를 해태하고, 노조조합장이 사임한 경우, 노동조합규약에 동 조합장의 직무를 대행할 자를 규정해 두고 있음에도 원고 자신이 주동하여 노조자치수습대책위원회를 구성하여 그 위원장으로 피선되어 근무시간중에도 노조활동을 벌여 운수업체인 소속회사의 업무에 지장을 초래하고 종업원들에게도 나쁜 영향을 끼쳐 소속회사가 취업규칙을 위반하고 고의로 회사업무능률을 저해하였으며 회사업무상의 지휘명령에 위반하였음을 이유로 원고를 징계해고 하였다면, 이는 원고의 노동조합 활동과는 관계없이 회사취업규칙에 의하여 사내질서를 유지하기 위한 사용자 고유의 징계권에 기하여 이루어진 정당한 징계권의 행사로 보아야 한다.', 'target_text': '원고가  주동하여 회사업무능률을 저해하고 회사업무상의 지휘명령에 위반하였다면 이에 따른 징계해고는 사내질서를 유지하기 위한 사용자 고유의 정당한 징계권의 행사로 보아야 한다.'}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "# CSV 파일 불러오기 (UTF-8 인코딩 사용, 필요시 'utf-8-sig' 또는 'euc-kr'로 변경)\n",
    "csv_path = \"extracted_documents.csv\"\n",
    "df = pd.read_csv(csv_path, encoding=\"utf-8-sig\")\n",
    "\n",
    "# DataFrame 컬럼 이름 변경: 원문과 요약 컬럼명을 일관성 있게 변경합니다.\n",
    "df = df.rename(columns={\"original_text\": \"input_text\", \"summary_text\": \"target_text\"})\n",
    "\n",
    "# 데이터 전처리 예시: 한국어 텍스트 정제를 위한 간단한 함수 추가\n",
    "def clean_korean_text(text):\n",
    "    # 불필요한 공백 제거, 특수문자 처리 등 추가적인 처리가 필요하면 여기에 구현\n",
    "    return text.strip()\n",
    "\n",
    "# 전처리 적용: 각 원문과 요약문에 대해 정제 함수를 적용\n",
    "df[\"input_text\"] = df[\"input_text\"].apply(clean_korean_text)\n",
    "df[\"target_text\"] = df[\"target_text\"].apply(clean_korean_text)\n",
    "\n",
    "# Pandas DataFrame을 Hugging Face Dataset으로 변환합니다.\n",
    "dataset = Dataset.from_pandas(df)\n",
    "print(dataset[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "414e3599c5d24db1b02f12da049cd2f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/24329 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenized Dataset Sample:\n",
      "{'input_text': '### 원문:\\n원고가 소속회사의 노동조합에서 분규가 발생하자 노조활동을 구실로 정상적인 근무를 해태하고, 노조조합장이 사임한 경우, 노동조합규약에 동 조합장의 직무를 대행할 자를 규정해 두고 있음에도 원고 자신이 주동하여 노조자치수습대책위원회를 구성하여 그 위원장으로 피선되어 근무시간중에도 노조활동을 벌여 운수업체인 소속회사의 업무에 지장을 초래하고 종업원들에게도 나쁜 영향을 끼쳐 소속회사가 취업규칙을 위반하고 고의로 회사업무능률을 저해하였으며 회사업무상의 지휘명령에 위반하였음을 이유로 원고를 징계해고 하였다면, 이는 원고의 노동조합 활동과는 관계없이 회사취업규칙에 의하여 사내질서를 유지하기 위한 사용자 고유의 징계권에 기하여 이루어진 정당한 징계권의 행사로 보아야 한다.\\n\\n### 요약:', 'target_text': '원고가  주동하여 회사업무능률을 저해하고 회사업무상의 지휘명령에 위반하였다면 이에 따른 징계해고는 사내질서를 유지하기 위한 사용자 고유의 정당한 징계권의 행사로 보아야 한다.'}\n"
     ]
    }
   ],
   "source": [
    "# 4. 전처리 함수 정의 (batched=True)\n",
    "def preprocess_function(examples):\n",
    "    \"\"\"\n",
    "    각 예제에 대해 프롬프트와 타겟 텍스트를 생성합니다.\n",
    "    - 'input_text' 컬럼에 저장된 원문 앞에 \"### 원문:\"과 \"### 요약:\" 구분자를 추가합니다.\n",
    "    - 'target_text' 컬럼은 그대로 요약문으로 사용합니다.\n",
    "    \"\"\"\n",
    "    prompts = []\n",
    "    targets = []\n",
    "    for document, summary in zip(examples[\"input_text\"], examples[\"target_text\"]):\n",
    "        prompt = \"### 원문:\\n\" + document + \"\\n\\n### 요약:\"\n",
    "        prompts.append(prompt)\n",
    "        targets.append(summary)\n",
    "    return {\"input_text\": prompts, \"target_text\": targets}\n",
    "\n",
    "# 5. 데이터셋에 전처리 함수 적용\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True)\n",
    "print(\"\\nTokenized Dataset Sample:\")\n",
    "print(tokenized_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 정보 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 샘플:\n",
      "                                       original_text  \\\n",
      "0  원고가 소속회사의 노동조합에서 분규가 발생하자 노조활동을 구실로 정상적인 근무를 해...   \n",
      "1  수출입업체인 원고가 의류제품을 제조ㆍ수출함에 있어 같은 그룹내 종합무역상사인 소외 ...   \n",
      "2  가등기담보권자가 제소전 화해조항에 따라 자기 명의로 소유권이전의 본등기를 경료한 후...   \n",
      "3  가. 부가가치세법 제22조 제3항 단서에 제1호와 제2호가 동시에 해당한다는 뜻은 ...   \n",
      "4  소득세법 제116조 제1항의 규정에 의하면 정부는 과세표준확정신고를 하여야 할 자에...   \n",
      "\n",
      "                                        summary_text  \n",
      "0  원고가  주동하여 회사업무능률을 저해하고 회사업무상의 지휘명령에 위반하였다면 이에 ...  \n",
      "1  수출입업체인 원고가 의류제품을 제조ㆍ수출함에 있어 소외 회사의 직수출실적을 지원하기...  \n",
      "2  가등기담보권자가 제소전 화해조항에 의해 자기 명의로 소유권이전의 본등기를 경료하고 ...  \n",
      "3  부가가치세법 제22조 제3항 단서에 제1호와 제2호가 동시에 해당한다는 의미는 제1...  \n",
      "4  소득세법 제116조 제1항에 따르면 정부는 과세표준확정신고를 해야 할 자에 대해 당...  \n",
      "\n",
      "데이터 정보:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 24329 entries, 0 to 24328\n",
      "Data columns (total 2 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   original_text  24329 non-null  object\n",
      " 1   summary_text   24329 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 380.3+ KB\n",
      "None\n",
      "\n",
      "숫자형 컬럼이 없습니다. 해당 시각화를 건너뜁니다.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# CSV 파일 불러오기 (파일 경로를 실제 경로로 수정하세요)\n",
    "csv_path = \"extracted_documents.csv\"\n",
    "df = pd.read_csv(csv_path, encoding=\"utf-8\")\n",
    "\n",
    "# 데이터프레임 구조와 컬럼 정보 출력\n",
    "print(\"데이터 샘플:\")\n",
    "print(df.head())\n",
    "print(\"\\n데이터 정보:\")\n",
    "print(df.info())\n",
    "\n",
    "# 숫자형 컬럼 추출\n",
    "numeric_cols = df.select_dtypes(include=[\"int64\", \"float64\"]).columns\n",
    "\n",
    "# 숫자형 컬럼이 존재하는지 확인\n",
    "if len(numeric_cols) == 0:\n",
    "    print(\"\\n숫자형 컬럼이 없습니다. 해당 시각화를 건너뜁니다.\")\n",
    "else:\n",
    "    # 1. 수치형 컬럼 분포: 히스토그램을 통해 각 수치형 컬럼의 분포를 파악\n",
    "    df[numeric_cols].hist(figsize=(12, 10), bins=20)\n",
    "    plt.suptitle(\"수치형 컬럼 히스토그램\")\n",
    "    plt.show()\n",
    "\n",
    "    # 2. 이상치 탐지를 위한 박스플롯: 각 수치형 컬럼의 분포와 이상치를 확인\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    for i, col in enumerate(numeric_cols, 1):\n",
    "        plt.subplot(1, len(numeric_cols), i)\n",
    "        sns.boxplot(x=df[col])\n",
    "        plt.title(f\"{col} 박스플롯\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # 3. 밀도 플롯: 수치형 컬럼의 분포를 부드러운 곡선으로 시각화\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for col in numeric_cols:\n",
    "        sns.kdeplot(df[col].dropna(), shade=True, label=col)\n",
    "    plt.title(\"수치형 컬럼 밀도 플롯\")\n",
    "    plt.xlabel(\"값\")\n",
    "    plt.ylabel(\"밀도\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⭐3. 모델 및 토크나이저 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wanted-1/miniconda3/envs/whisper/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer loaded with 4-bit quantization and LoRA applied.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "def load_model_and_tokenizer(model_name=\"google/gemma-7b-it\"):\n",
    "    \"\"\"\n",
    "    4비트 양자화와 PEFT(LoRA)를 적용하여 모델과 토크나이저를 로드합니다.\n",
    "    4비트 양자화 설정은 BitsAndBytesConfig에서 load_in_4bit=True를 사용하며,\n",
    "    추가 설정으로 연산 dtype, 이중 양자화 사용 여부, 양자화 타입을 지정합니다.\n",
    "    \"\"\"\n",
    "    # 4-bit 양자화 설정\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_compute_dtype=torch.float16,  # 연산 시 사용할 데이터 타입\n",
    "        bnb_4bit_use_double_quant=True,         # 이중 양자화 사용 (메모리 효율 개선)\n",
    "        bnb_4bit_quant_type=\"nf4\"               # 양자화 타입: \"nf4\" 또는 \"fp4\" 중 선택 (일반적으로 \"nf4\" 추천)\n",
    "    )\n",
    "    # device_map = \"auto\"\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    # 토크나이저와 양자화된 모델 로드\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=bnb_config,\n",
    "        #device_map=device_map\n",
    "    )\n",
    "    \n",
    "    # 4-bit 양자화된 모델은 기본적으로 파인튜닝이 불가능하므로, LoRA 어댑터를 추가합니다.\n",
    "    lora_config = LoraConfig(\n",
    "        r=8,\n",
    "        lora_alpha=32,\n",
    "        target_modules=[\"q_proj\", \"v_proj\"],  # 모델 구조에 맞게 조정 (예: Gemma 모델의 경우)\n",
    "        lora_dropout=0.1,\n",
    "        bias=\"none\",\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "    )\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "# 사용 예시:\n",
    "model, tokenizer = load_model_and_tokenizer(\"google/gemma-7b-it\")\n",
    "print(\"Model and tokenizer loaded with 4-bit quantization and LoRA applied.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⭐4. 모델 튜닝하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM, \n",
    "    BitsAndBytesConfig, \n",
    "    TrainingArguments, \n",
    "    Trainer, \n",
    "    DataCollatorForSeq2Seq\n",
    ")\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) 모델 및 토크나이저 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "def load_model_and_tokenizer(model_name=\"google/gemma-7b-it\"):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "# def load_model_and_tokenizer(model_name=\"google/gemma-7b-it\"):\n",
    "#     \"\"\"\n",
    "#     BitsAndBytesConfig를 사용하여 8-bit quantization과 CPU offload를 적용해 모델을 로드합니다.\n",
    "#     \"\"\"\n",
    "#     bnb_config = BitsAndBytesConfig(\n",
    "#         load_in_8bit=True,           \n",
    "#         llm_int8_threshold=6.0,\n",
    "#         llm_int8_has_fp16_weight=False,\n",
    "#         llm_int8_enable_fp32_cpu_offload=True  # CPU offload 활성화\n",
    "#     )\n",
    "    \n",
    "#     # device_map은 \"auto\"를 사용하여 내부적으로 offload 전략이 적용되도록 함\n",
    "#     device_map = \"auto\"\n",
    "    \n",
    "#     tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "#     model = AutoModelForCausalLM.from_pretrained(\n",
    "#         model_name,\n",
    "#         quantization_config=bnb_config,\n",
    "#         device_map=device_map\n",
    "#     )\n",
    "#     return model, tokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ⭐2) 데이터셋 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(csv_path, encoding=\"utf-8\"):\n",
    "    \"\"\"\n",
    "    CSV 파일을 불러와서 데이터셋을 구축합니다.\n",
    "    CSV 파일에는 'original_text'와 'summary_text' 컬럼이 있다고 가정하며,\n",
    "    이를 'input_text'와 'target_text'로 재정의합니다.\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    from datasets import Dataset\n",
    "    \n",
    "    df = pd.read_csv(csv_path, encoding=encoding)\n",
    "    df = df.rename(columns={\"original_text\": \"input_text\", \"summary_text\": \"target_text\"})\n",
    "    dataset = Dataset.from_pandas(df)\n",
    "    return dataset\n",
    "\n",
    "def tokenize_dataset(dataset, tokenizer, max_input_length=1024, max_target_length=256):\n",
    "    \"\"\"\n",
    "    데이터셋의 각 예제에 대해 입력과 타겟 텍스트를 토큰화합니다.\n",
    "    \"\"\"\n",
    "    def tokenize_function(example):\n",
    "        model_inputs = tokenizer(example[\"input_text\"], max_length=max_input_length, truncation=True)\n",
    "        with tokenizer.as_target_tokenizer():\n",
    "            labels = tokenizer(example[\"target_text\"], max_length=max_target_length, truncation=True)\n",
    "        model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "        return model_inputs\n",
    "    \n",
    "    tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=dataset.column_names)\n",
    "    return tokenized_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ⭐3) 학습 설정 및 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, tokenizer, tokenized_dataset, output_dir=\"./results\", epochs=3, seed=42):\n",
    "    \"\"\"\n",
    "    토큰화된 데이터셋을 학습용과 평가용으로 분할하고, Trainer를 사용하여 모델을 fine-tuning합니다.\n",
    "    \"\"\"\n",
    "    # 데이터셋 분할\n",
    "    split_dataset = tokenized_dataset.train_test_split(test_size=0.1, seed=seed)\n",
    "    train_dataset = split_dataset[\"train\"]\n",
    "    eval_dataset = split_dataset[\"test\"]\n",
    "    \n",
    "    # Data Collator 설정\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "    \n",
    "    # TrainingArguments 설정\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        learning_rate=5e-5,\n",
    "        num_train_epochs=epochs,\n",
    "        weight_decay=0.01,\n",
    "        fp16=True,\n",
    "        logging_steps=50,\n",
    "        save_total_limit=3,\n",
    "        seed=seed\n",
    "    )\n",
    "    \n",
    "    # Trainer 객체 생성\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator\n",
    "    )\n",
    "    \n",
    "    # 학습 실행\n",
    "    trainer.train()\n",
    "    \n",
    "    # 학습 완료 후 모델 저장\n",
    "    trainer.save_model(os.path.join(output_dir, \"gemma_finetuned_final\"))\n",
    "    \n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ⭐4) 메인 함수: 전체 파이프라인 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.48s/it]\n",
      "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]/home/wanted-1/miniconda3/envs/whisper/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3970: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 1000/1000 [00:00<00:00, 9289.42 examples/s]\n",
      "/home/wanted-1/miniconda3/envs/whisper/lib/python3.10/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_2181768/3916027822.py:27: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 288.00 MiB. GPU 0 has a total capacity of 23.63 GiB of which 265.00 MiB is free. Including non-PyTorch memory, this process has 23.24 GiB memory in use. Of the allocated memory 22.78 GiB is allocated by PyTorch, and 26.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m     train_model(model, tokenizer, tokenized_dataset, output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./results\u001b[39m\u001b[38;5;124m\"\u001b[39m, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 19\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 16\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m tokenized_dataset \u001b[38;5;241m=\u001b[39m tokenize_dataset(raw_dataset, tokenizer)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# 3. 모델 튜닝\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenized_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./results\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 27\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, tokenizer, tokenized_dataset, output_dir, epochs, seed)\u001b[0m\n\u001b[1;32m     14\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[1;32m     15\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39moutput_dir,\n\u001b[1;32m     16\u001b[0m     evaluation_strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     23\u001b[0m     seed\u001b[38;5;241m=\u001b[39mseed\n\u001b[1;32m     24\u001b[0m )\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Trainer 객체 생성\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_collator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_collator\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# 학습 실행\u001b[39;00m\n\u001b[1;32m     37\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[0;32m~/miniconda3/envs/whisper/lib/python3.10/site-packages/transformers/utils/deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/whisper/lib/python3.10/site-packages/transformers/trainer.py:612\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, processing_class, model_init, compute_loss_func, compute_metrics, callbacks, optimizers, optimizer_cls_and_kwargs, preprocess_logits_for_metrics)\u001b[0m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;66;03m# Bnb Quantized models doesn't support `.to` operation.\u001b[39;00m\n\u001b[1;32m    608\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    609\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mplace_model_on_device\n\u001b[1;32m    610\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantization_method\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m QuantizationMethod\u001b[38;5;241m.\u001b[39mBITS_AND_BYTES\n\u001b[1;32m    611\u001b[0m ):\n\u001b[0;32m--> 612\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_move_model_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    614\u001b[0m \u001b[38;5;66;03m# Force n_gpu to 1 to avoid DataParallel as MP will manage the GPUs\u001b[39;00m\n\u001b[1;32m    615\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_model_parallel:\n",
      "File \u001b[0;32m~/miniconda3/envs/whisper/lib/python3.10/site-packages/transformers/trainer.py:899\u001b[0m, in \u001b[0;36mTrainer._move_model_to_device\u001b[0;34m(self, model, device)\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_move_model_to_device\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, device):\n\u001b[0;32m--> 899\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    900\u001b[0m     \u001b[38;5;66;03m# Moving a model to an XLA device disconnects the tied weights, so we have to retie them.\u001b[39;00m\n\u001b[1;32m    901\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mparallel_mode \u001b[38;5;241m==\u001b[39m ParallelMode\u001b[38;5;241m.\u001b[39mTPU \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtie_weights\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/miniconda3/envs/whisper/lib/python3.10/site-packages/transformers/modeling_utils.py:3162\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3157\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[1;32m   3158\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3159\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3160\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3161\u001b[0m         )\n\u001b[0;32m-> 3162\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/whisper/lib/python3.10/site-packages/torch/nn/modules/module.py:1343\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1340\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1341\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1343\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/whisper/lib/python3.10/site-packages/torch/nn/modules/module.py:903\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    902\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 903\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    905\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    906\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    907\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    908\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    913\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    914\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/whisper/lib/python3.10/site-packages/torch/nn/modules/module.py:903\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    902\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 903\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    905\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    906\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    907\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    908\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    913\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    914\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 903 (2 times)]\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/whisper/lib/python3.10/site-packages/torch/nn/modules/module.py:903\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    902\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 903\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    905\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    906\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    907\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    908\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    913\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    914\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/whisper/lib/python3.10/site-packages/torch/nn/modules/module.py:930\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    926\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    927\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    928\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    929\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 930\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    931\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    933\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/whisper/lib/python3.10/site-packages/torch/nn/modules/module.py:1329\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1323\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1324\u001b[0m             device,\n\u001b[1;32m   1325\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1326\u001b[0m             non_blocking,\n\u001b[1;32m   1327\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1328\u001b[0m         )\n\u001b[0;32m-> 1329\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1331\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1332\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1333\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1334\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1335\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 288.00 MiB. GPU 0 has a total capacity of 23.63 GiB of which 265.00 MiB is free. Including non-PyTorch memory, this process has 23.24 GiB memory in use. Of the allocated memory 22.78 GiB is allocated by PyTorch, and 26.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # 0. GPU 설정 (필요한 경우 환경 변수나 코드 내 device 설정)\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Using device:\", device)\n",
    "    \n",
    "    # 1. 모델 및 토크나이저 로드 (모델은 이미 device_map=\"auto\"로 올바른 디바이스에 로드됨)\n",
    "    model, tokenizer = load_model_and_tokenizer(\"google/gemma-7b-it\")\n",
    "    # model.to(device)  # 8-bit 모델에서는 이 호출이 필요하지 않으며, 에러가 발생합니다.\n",
    "    \n",
    "    # 2. 데이터셋 준비 및 토큰화\n",
    "    csv_path = \"extracted_documents.csv\"  # CSV 파일 경로 지정\n",
    "    raw_dataset = prepare_dataset(csv_path, encoding=\"utf-8\")\n",
    "    tokenized_dataset = tokenize_dataset(raw_dataset, tokenizer)\n",
    "    \n",
    "    # 3. 모델 튜닝\n",
    "    train_model(model, tokenizer, tokenized_dataset, output_dir=\"./results\", epochs=3, seed=42)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m     train_model(model, tokenizer, tokenized_dataset, output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./results\u001b[39m\u001b[38;5;124m\"\u001b[39m, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 19\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmain\u001b[39m():\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# 0. GPU 설정 (필요한 경우 환경 변수나 코드 내 device 설정)\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m     device \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing device:\u001b[39m\u001b[38;5;124m\"\u001b[39m, device)\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# 1. 모델 및 토크나이저 로드 (모델은 이미 device_map=\"auto\"로 올바른 디바이스에 로드됨)\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # 0. GPU 설정 (필요한 경우 환경 변수나 코드 내 device 설정)\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Using device:\", device)\n",
    "    \n",
    "    # 1. 모델 및 토크나이저 로드 (모델은 이미 device_map=\"auto\"로 올바른 디바이스에 로드됨)\n",
    "    model, tokenizer = load_model_and_tokenizer(\"google/gemma-7b-it\")\n",
    "    # model.to(device)  # 8-bit 모델에서는 이 호출이 필요하지 않으며, 에러가 발생합니다.\n",
    "    \n",
    "    # 2. 데이터셋 준비 및 토큰화\n",
    "    csv_path = \"extracted_documents.csv\"  # CSV 파일 경로 지정\n",
    "    raw_dataset = prepare_dataset(csv_path, encoding=\"utf-8\")\n",
    "    tokenized_dataset = tokenize_dataset(raw_dataset, tokenizer)\n",
    "    \n",
    "    # 3. 모델 튜닝\n",
    "    train_model(model, tokenizer, tokenized_dataset, output_dir=\"./results\", epochs=3, seed=42)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 프롬프트 및 하이퍼파라미터 설정하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 28\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# 프롬프트 예시 (경제/금융 주제)\u001b[39;00m\n\u001b[1;32m     22\u001b[0m sample_input \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m### 원문:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m최근 한국 경제는 글로벌 경기 둔화와 내수 부진 등으로 어려움을 겪고 있으며, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m정부와 기업들은 다양한 정책과 혁신을 통해 위기를 극복하고자 노력하고 있습니다.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m### 요약:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     27\u001b[0m )\n\u001b[0;32m---> 28\u001b[0m sample_summary \u001b[38;5;241m=\u001b[39m generate_summary(sample_input, \u001b[43mmodel\u001b[49m, tokenizer, max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m150\u001b[39m)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerated Summary:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, sample_summary)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# def generate_summary(input_text, model, tokenizer, max_new_tokens=150):\n",
    "#     \"\"\"\n",
    "#     Fine-tuning된 모델을 이용해 프롬프트에 따른 요약을 생성하는 함수입니다.\n",
    "#     생성 파라미터(temperature, top_p 등)는 실험 결과에 따라 조정할 수 있습니다.\n",
    "#     \"\"\"\n",
    "#     inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "#     with torch.no_grad():\n",
    "#         outputs = model.generate(\n",
    "#             input_ids=inputs[\"input_ids\"],\n",
    "#             attention_mask=inputs[\"attention_mask\"],\n",
    "#             max_new_tokens=max_new_tokens,\n",
    "#             do_sample=True,              # 무작위성 유지 (필요에 따라 결정적 생성도 고려)\n",
    "#             temperature=0.7,\n",
    "#             top_p=0.8,\n",
    "#             repetition_penalty=1.2,\n",
    "#             no_repeat_ngram_size=3,\n",
    "#             pad_token_id=tokenizer.pad_token_id or tokenizer.eos_token_id\n",
    "#         )\n",
    "#     return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# # 프롬프트 예시 (경제/금융 주제)\n",
    "# sample_input = (\n",
    "#     \"### 원문:\\n\"\n",
    "#     \"최근 한국 경제는 글로벌 경기 둔화와 내수 부진 등으로 어려움을 겪고 있으며, \"\n",
    "#     \"정부와 기업들은 다양한 정책과 혁신을 통해 위기를 극복하고자 노력하고 있습니다.\\n\\n\"\n",
    "#     \"### 요약:\"\n",
    "# )\n",
    "# sample_summary = generate_summary(sample_input, model, tokenizer, max_new_tokens=150)\n",
    "# print(\"Generated Summary:\\n\", sample_summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 출력 결과 확인 후 프롬프트 및 하이퍼 파라미터 조정하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def evaluate_and_adjust():\n",
    "#     \"\"\"\n",
    "#     생성된 결과를 평가하고, 프롬프트 문구나 생성 파라미터를 조정하는 반복 과정을 수행합니다.\n",
    "#     이 단계는 수동 또는 자동화된 평가 루프를 통해 진행할 수 있습니다.\n",
    "#     \"\"\"\n",
    "#     # 예시: 생성 결과 평가 (실제 평가 기준은 ROUGE, BLEU 등 메트릭을 활용할 수 있음)\n",
    "#     print(\"출력 결과를 확인하고, 프롬프트 및 파라미터를 조정하세요.\")\n",
    "#     # 필요 시, 재생성 및 파라미터 변경 로직을 추가합니다.\n",
    "\n",
    "# # 평가 및 조정 호출 예시\n",
    "# evaluate_and_adjust()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 튜닝 루프 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tuning_loop(model, tokenizer, trainer, iterations=3):\n",
    "#     \"\"\"\n",
    "#     4~6단계를 반복하는 튜닝 루프입니다.\n",
    "#     iterations 값은 실험에 따라 조정합니다.\n",
    "#     각 반복에서 모델의 출력 결과를 평가하고,\n",
    "#     필요 시 프롬프트 문구 및 하이퍼파라미터를 조정하여 최적의 결과를 도출합니다.\n",
    "#     \"\"\"\n",
    "#     for i in range(iterations):\n",
    "#         print(f\"\\n=== Tuning iteration {i+1} ===\")\n",
    "#         # 프롬프트 및 하이퍼파라미터 설정\n",
    "#         sample_input = (\n",
    "#             \"### 원문:\\n\"\n",
    "#             \"최근 한국 경제는 글로벌 경기 둔화와 내수 부진 등으로 어려움을 겪고 있으며, \"\n",
    "#             \"정부와 기업들은 다양한 정책과 혁신을 통해 위기를 극복하고자 노력하고 있습니다.\\n\\n\"\n",
    "#             \"### 요약:\"\n",
    "#         )\n",
    "#         output = generate_summary(sample_input, model, tokenizer, max_new_tokens=150)\n",
    "#         print(\"Generated Output:\\n\", output)\n",
    "        \n",
    "#         # 평가 및 필요 시 파라미터, 프롬프트 조정 (여기서는 단순히 출력 확인)\n",
    "#         evaluate_and_adjust()\n",
    "    \n",
    "#     print(\"최종 튜닝 완료.\")\n",
    "\n",
    "# # 튜닝 루프 실행 예시\n",
    "# tuning_loop(model, tokenizer, trainer, iterations=3)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "whisper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
