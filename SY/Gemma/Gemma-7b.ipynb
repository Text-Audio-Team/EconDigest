{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gemma-7b í™œìš©/ ìš”ì•½ ëª¨ë¸ ë§Œë“¤ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "#import os\n",
    "# # ì„œë²„ì˜ GPU 1ë²ˆ ì‚¬ìš©í•˜ê²Œ ë§Œë“¤ê¸° \n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # 1ë²ˆ GPUë§Œ ë…¸ì¶œë¨\n",
    "import torch\n",
    "\n",
    "# ì›í•˜ëŠ” GPUë¥¼ ì§€ì • (ì˜ˆ: ë‘ ë²ˆì§¸ GPU ì‚¬ìš©)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ë°ì´í„°ì…‹ êµ¬ì¶•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "012ada7c0222471c8b50d1de99cb591d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['name', 'delivery_date', 'documents'],\n",
      "        num_rows: 24329\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import zipfile\n",
    "from datasets import load_dataset\n",
    "\n",
    "def load_and_build_dataset_from_zip(zip_path, extracted_dir):\n",
    "    \"\"\"\n",
    "    zip íŒŒì¼ ë‚´ì˜ JSON íŒŒì¼ë“¤ì„ ë™ì ìœ¼ë¡œ ê²€ìƒ‰í•˜ì—¬ ë°ì´í„°ì…‹ì„ êµ¬ì„±í•©ë‹ˆë‹¤.\n",
    "    \n",
    "    1. ì§€ì •ëœ ê²½ë¡œì— ì••ì¶• íŒŒì¼ì„ í•´ì œí•©ë‹ˆë‹¤.\n",
    "    2. glob ëª¨ë“ˆì„ ì‚¬ìš©í•´ ëª¨ë“  JSON íŒŒì¼ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤.\n",
    "    3. ì •ê·œí‘œí˜„ì‹ì„ í™œìš©í•˜ì—¬ íŒŒì¼ëª…ì— 'train' (ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ì—†ì´)ì´ í¬í•¨ëœ íŒŒì¼ë§Œ í•„í„°ë§í•©ë‹ˆë‹¤.\n",
    "    4. í•„í„°ë§ëœ íŒŒì¼ë“¤ì„ ì´ìš©í•´ Hugging Faceì˜ load_dataset í•¨ìˆ˜ë¡œ ë°ì´í„°ì…‹ì„ êµ¬ì„±í•©ë‹ˆë‹¤.\n",
    "    \n",
    "    :param zip_path: ì••ì¶• íŒŒì¼ì˜ ê²½ë¡œ\n",
    "    :param extracted_dir: ì••ì¶• í•´ì œí•  ë””ë ‰í† ë¦¬\n",
    "    :return: êµ¬ì„±ëœ ë°ì´í„°ì…‹\n",
    "    \"\"\"\n",
    "    # ì••ì¶• í•´ì œí•  ë””ë ‰í† ë¦¬ê°€ ì—†ìœ¼ë©´ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "    if not os.path.exists(extracted_dir):\n",
    "        os.makedirs(extracted_dir)\n",
    "    \n",
    "    # zip íŒŒì¼ì„ ì—´ì–´ ì§€ì •ëœ ë””ë ‰í† ë¦¬ì— ì••ì¶• í•´ì œ\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extracted_dir)\n",
    "    \n",
    "    # glob ëª¨ë“ˆì„ ì‚¬ìš©í•˜ì—¬ extracted_dir ë‚´ì˜ ëª¨ë“  JSON íŒŒì¼ ê²€ìƒ‰\n",
    "    json_files = glob.glob(os.path.join(extracted_dir, \"*.json\"))\n",
    "    \n",
    "    # ì •ê·œí‘œí˜„ì‹ì„ ì‚¬ìš©í•˜ì—¬ íŒŒì¼ëª…ì— 'train'ì´ í¬í•¨ëœ íŒŒì¼ë§Œ í•„í„°ë§ (ëŒ€ì†Œë¬¸ì ë¬´ì‹œ)\n",
    "    train_files = [f for f in json_files if re.search(r\"train\", os.path.basename(f), re.IGNORECASE)]\n",
    "    \n",
    "    if not train_files:\n",
    "        raise ValueError(\"ì••ì¶• í•´ì œëœ ë””ë ‰í† ë¦¬ì—ì„œ 'train'ì´ í¬í•¨ëœ JSON íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "    \n",
    "    # validation íŒŒì¼ë„ í•„ìš”í•˜ë©´ ë¹„ìŠ·í•œ ë°©ì‹ìœ¼ë¡œ í•„í„°ë§ ê°€ëŠ¥ (ì˜ˆì‹œ)\n",
    "    validation_files = [f for f in json_files if re.search(r\"validation\", os.path.basename(f), re.IGNORECASE)]\n",
    "    \n",
    "    # ë°ì´í„° íŒŒì¼ ë”•ì…”ë„ˆë¦¬ ìƒì„±: load_datasetì€ ë¦¬ìŠ¤íŠ¸ë¡œ ì „ë‹¬ë°›ì€ ì—¬ëŸ¬ íŒŒì¼ë“¤ì„ ìë™ìœ¼ë¡œ ë³‘í•©í•©ë‹ˆë‹¤.\n",
    "    data_files = {\"train\": train_files}\n",
    "    if validation_files:\n",
    "        data_files[\"validation\"] = validation_files\n",
    "    \n",
    "    # JSON íŒŒì¼ë“¤ë¡œë¶€í„° ë°ì´í„°ì…‹ êµ¬ì„±\n",
    "    dataset = load_dataset(\"json\", data_files=data_files)\n",
    "    return dataset\n",
    "\n",
    "# ì‚¬ìš© ì˜ˆì‹œ:\n",
    "zip_path = \"/home/wanted-1/potenup-workspace/Project/project3/team2/í•™ìŠµë°ì´í„°/06_ë¬¸ì„œìš”ì•½ í…ìŠ¤íŠ¸/Training/ë²•ë¥ _train_original.zip\"\n",
    "extracted_dir = \"/home/wanted-1/potenup-workspace/Project/project3/team2/í•™ìŠµë°ì´í„°/06_ë¬¸ì„œìš”ì•½ í…ìŠ¤íŠ¸/Training/extracted\"\n",
    "\n",
    "dataset = load_and_build_dataset_from_zip(zip_path, extracted_dir)\n",
    "print(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_original_and_summary(document):\n",
    "    \"\"\"\n",
    "    ì£¼ì–´ì§„ document ë”•ì…”ë„ˆë¦¬ì—ì„œ ì›ë¬¸ê³¼ ìš”ì•½ë¬¸ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.\n",
    "    \n",
    "    ì›ë¬¸:\n",
    "        - 'text' í•„ë“œëŠ” ë¦¬ìŠ¤íŠ¸ ì•ˆì— ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\n",
    "        - ê° ë‚´ë¶€ ë”•ì…”ë„ˆë¦¬ì˜ 'sentence' ê°’ì„ ì¶”ì¶œí•˜ì—¬ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ ê²°í•©í•©ë‹ˆë‹¤.\n",
    "    ìš”ì•½ë¬¸:\n",
    "        - 'abstractive' í•„ë“œì— ìš”ì•½ë¬¸ì´ ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸ë¡œ ì œê³µë˜ë©°,\n",
    "          ì²« ë²ˆì§¸ ìš”ì†Œë¥¼ ìµœì¢… ìš”ì•½ë¬¸ìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    # ì›ë¬¸ ì¶”ì¶œ: 'text' í•„ë“œì˜ ê° ë¬¸ì¥ì„ ê²°í•©í•©ë‹ˆë‹¤.\n",
    "    original_sentences = []\n",
    "    for block in document.get(\"text\", []):\n",
    "        for sentence_info in block:\n",
    "            sentence = sentence_info.get(\"sentence\", \"\")\n",
    "            if sentence:\n",
    "                original_sentences.append(sentence.strip())\n",
    "    original_text = \" \".join(original_sentences)\n",
    "    \n",
    "    # ìš”ì•½ë¬¸ ì¶”ì¶œ: 'abstractive' í•„ë“œì˜ ì²« ë²ˆì§¸ ìš”ì†Œ ì‚¬ìš©\n",
    "    summary_list = document.get(\"abstractive\", [])\n",
    "    summary_text = summary_list[0].strip() if summary_list else \"\"\n",
    "    \n",
    "    return original_text, summary_text\n",
    "\n",
    "# ì „ì²´ ë°ì´í„°ì…‹ì˜ ëª¨ë“  ë¬¸ì„œì—ì„œ ì›ë¬¸ê³¼ ìš”ì•½ë¬¸ì„ ì¶”ì¶œí•˜ëŠ” ë°˜ë³µë¬¸\n",
    "# dataset['train']['documents'] ê°€ ëª¨ë“  ë¬¸ì„œë¥¼ í¬í•¨í•˜ëŠ” ë¦¬ìŠ¤íŠ¸ë¼ê³  ê°€ì •í•©ë‹ˆë‹¤.\n",
    "all_originals = []\n",
    "all_summaries = []\n",
    "\n",
    "for document in dataset['train']['documents']:\n",
    "    original_text, summary_text = extract_original_and_summary(document)\n",
    "    all_originals.append(original_text)\n",
    "    all_summaries.append(summary_text)\n",
    "\n",
    "# # ì¶”ì¶œëœ ê²°ê³¼ë¥¼ ë¬¸ì„œë³„ë¡œ ì¶œë ¥í•©ë‹ˆë‹¤.\n",
    "# for idx, (orig, summ) in enumerate(zip(all_originals, all_summaries)):\n",
    "#     print(f\"Document {idx+1}\")\n",
    "#     print(\"Original Text:\")\n",
    "#     print(orig)\n",
    "#     print(\"Summary:\")\n",
    "#     print(summ)\n",
    "#     print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ì˜ˆë¥¼ ë“¤ì–´, ì „ì²´ ë¬¸ì„œì—ì„œ ì¶”ì¶œí•œ ì›ë¬¸ê³¼ ìš”ì•½ë¬¸ ë¦¬ìŠ¤íŠ¸ê°€ ìˆë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤.\n",
    "data = {\n",
    "    \"original_text\": all_originals,\n",
    "    \"summary_text\": all_summaries\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# DataFrame ë‚´ìš© í™•ì¸\n",
    "# print(df.head())\n",
    "\n",
    "# CSV íŒŒì¼ë¡œ ì €ì¥í•˜ê¸°\n",
    "df.to_csv(\"extracted_documents.csv\", index=False, encoding=\"utf-8-sig\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ë°ì´í„°ì…‹ í¬ë§·íŒ…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_text': 'ì›ê³ ê°€ ì†Œì†íšŒì‚¬ì˜ ë…¸ë™ì¡°í•©ì—ì„œ ë¶„ê·œê°€ ë°œìƒí•˜ì ë…¸ì¡°í™œë™ì„ êµ¬ì‹¤ë¡œ ì •ìƒì ì¸ ê·¼ë¬´ë¥¼ í•´íƒœí•˜ê³ , ë…¸ì¡°ì¡°í•©ì¥ì´ ì‚¬ì„í•œ ê²½ìš°, ë…¸ë™ì¡°í•©ê·œì•½ì— ë™ ì¡°í•©ì¥ì˜ ì§ë¬´ë¥¼ ëŒ€í–‰í•  ìë¥¼ ê·œì •í•´ ë‘ê³  ìˆìŒì—ë„ ì›ê³  ìì‹ ì´ ì£¼ë™í•˜ì—¬ ë…¸ì¡°ìì¹˜ìˆ˜ìŠµëŒ€ì±…ìœ„ì›íšŒë¥¼ êµ¬ì„±í•˜ì—¬ ê·¸ ìœ„ì›ì¥ìœ¼ë¡œ í”¼ì„ ë˜ì–´ ê·¼ë¬´ì‹œê°„ì¤‘ì—ë„ ë…¸ì¡°í™œë™ì„ ë²Œì—¬ ìš´ìˆ˜ì—…ì²´ì¸ ì†Œì†íšŒì‚¬ì˜ ì—…ë¬´ì— ì§€ì¥ì„ ì´ˆë˜í•˜ê³  ì¢…ì—…ì›ë“¤ì—ê²Œë„ ë‚˜ìœ ì˜í–¥ì„ ë¼ì³ ì†Œì†íšŒì‚¬ê°€ ì·¨ì—…ê·œì¹™ì„ ìœ„ë°˜í•˜ê³  ê³ ì˜ë¡œ íšŒì‚¬ì—…ë¬´ëŠ¥ë¥ ì„ ì €í•´í•˜ì˜€ìœ¼ë©° íšŒì‚¬ì—…ë¬´ìƒì˜ ì§€íœ˜ëª…ë ¹ì— ìœ„ë°˜í•˜ì˜€ìŒì„ ì´ìœ ë¡œ ì›ê³ ë¥¼ ì§•ê³„í•´ê³  í•˜ì˜€ë‹¤ë©´, ì´ëŠ” ì›ê³ ì˜ ë…¸ë™ì¡°í•© í™œë™ê³¼ëŠ” ê´€ê³„ì—†ì´ íšŒì‚¬ì·¨ì—…ê·œì¹™ì— ì˜í•˜ì—¬ ì‚¬ë‚´ì§ˆì„œë¥¼ ìœ ì§€í•˜ê¸° ìœ„í•œ ì‚¬ìš©ì ê³ ìœ ì˜ ì§•ê³„ê¶Œì— ê¸°í•˜ì—¬ ì´ë£¨ì–´ì§„ ì •ë‹¹í•œ ì§•ê³„ê¶Œì˜ í–‰ì‚¬ë¡œ ë³´ì•„ì•¼ í•œë‹¤.', 'target_text': 'ì›ê³ ê°€  ì£¼ë™í•˜ì—¬ íšŒì‚¬ì—…ë¬´ëŠ¥ë¥ ì„ ì €í•´í•˜ê³  íšŒì‚¬ì—…ë¬´ìƒì˜ ì§€íœ˜ëª…ë ¹ì— ìœ„ë°˜í•˜ì˜€ë‹¤ë©´ ì´ì— ë”°ë¥¸ ì§•ê³„í•´ê³ ëŠ” ì‚¬ë‚´ì§ˆì„œë¥¼ ìœ ì§€í•˜ê¸° ìœ„í•œ ì‚¬ìš©ì ê³ ìœ ì˜ ì •ë‹¹í•œ ì§•ê³„ê¶Œì˜ í–‰ì‚¬ë¡œ ë³´ì•„ì•¼ í•œë‹¤.'}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "# CSV íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸° (UTF-8 ì¸ì½”ë”© ì‚¬ìš©, í•„ìš”ì‹œ 'utf-8-sig' ë˜ëŠ” 'euc-kr'ë¡œ ë³€ê²½)\n",
    "csv_path = \"extracted_documents.csv\"\n",
    "df = pd.read_csv(csv_path, encoding=\"utf-8-sig\")\n",
    "\n",
    "# DataFrame ì»¬ëŸ¼ ì´ë¦„ ë³€ê²½: ì›ë¬¸ê³¼ ìš”ì•½ ì»¬ëŸ¼ëª…ì„ ì¼ê´€ì„± ìˆê²Œ ë³€ê²½í•©ë‹ˆë‹¤.\n",
    "df = df.rename(columns={\"original_text\": \"input_text\", \"summary_text\": \"target_text\"})\n",
    "\n",
    "# ë°ì´í„° ì „ì²˜ë¦¬ ì˜ˆì‹œ: í•œêµ­ì–´ í…ìŠ¤íŠ¸ ì •ì œë¥¼ ìœ„í•œ ê°„ë‹¨í•œ í•¨ìˆ˜ ì¶”ê°€\n",
    "def clean_korean_text(text):\n",
    "    # ë¶ˆí•„ìš”í•œ ê³µë°± ì œê±°, íŠ¹ìˆ˜ë¬¸ì ì²˜ë¦¬ ë“± ì¶”ê°€ì ì¸ ì²˜ë¦¬ê°€ í•„ìš”í•˜ë©´ ì—¬ê¸°ì— êµ¬í˜„\n",
    "    return text.strip()\n",
    "\n",
    "# ì „ì²˜ë¦¬ ì ìš©: ê° ì›ë¬¸ê³¼ ìš”ì•½ë¬¸ì— ëŒ€í•´ ì •ì œ í•¨ìˆ˜ë¥¼ ì ìš©\n",
    "df[\"input_text\"] = df[\"input_text\"].apply(clean_korean_text)\n",
    "df[\"target_text\"] = df[\"target_text\"].apply(clean_korean_text)\n",
    "\n",
    "# Pandas DataFrameì„ Hugging Face Datasetìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\n",
    "dataset = Dataset.from_pandas(df)\n",
    "print(dataset[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "414e3599c5d24db1b02f12da049cd2f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/24329 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenized Dataset Sample:\n",
      "{'input_text': '### ì›ë¬¸:\\nì›ê³ ê°€ ì†Œì†íšŒì‚¬ì˜ ë…¸ë™ì¡°í•©ì—ì„œ ë¶„ê·œê°€ ë°œìƒí•˜ì ë…¸ì¡°í™œë™ì„ êµ¬ì‹¤ë¡œ ì •ìƒì ì¸ ê·¼ë¬´ë¥¼ í•´íƒœí•˜ê³ , ë…¸ì¡°ì¡°í•©ì¥ì´ ì‚¬ì„í•œ ê²½ìš°, ë…¸ë™ì¡°í•©ê·œì•½ì— ë™ ì¡°í•©ì¥ì˜ ì§ë¬´ë¥¼ ëŒ€í–‰í•  ìë¥¼ ê·œì •í•´ ë‘ê³  ìˆìŒì—ë„ ì›ê³  ìì‹ ì´ ì£¼ë™í•˜ì—¬ ë…¸ì¡°ìì¹˜ìˆ˜ìŠµëŒ€ì±…ìœ„ì›íšŒë¥¼ êµ¬ì„±í•˜ì—¬ ê·¸ ìœ„ì›ì¥ìœ¼ë¡œ í”¼ì„ ë˜ì–´ ê·¼ë¬´ì‹œê°„ì¤‘ì—ë„ ë…¸ì¡°í™œë™ì„ ë²Œì—¬ ìš´ìˆ˜ì—…ì²´ì¸ ì†Œì†íšŒì‚¬ì˜ ì—…ë¬´ì— ì§€ì¥ì„ ì´ˆë˜í•˜ê³  ì¢…ì—…ì›ë“¤ì—ê²Œë„ ë‚˜ìœ ì˜í–¥ì„ ë¼ì³ ì†Œì†íšŒì‚¬ê°€ ì·¨ì—…ê·œì¹™ì„ ìœ„ë°˜í•˜ê³  ê³ ì˜ë¡œ íšŒì‚¬ì—…ë¬´ëŠ¥ë¥ ì„ ì €í•´í•˜ì˜€ìœ¼ë©° íšŒì‚¬ì—…ë¬´ìƒì˜ ì§€íœ˜ëª…ë ¹ì— ìœ„ë°˜í•˜ì˜€ìŒì„ ì´ìœ ë¡œ ì›ê³ ë¥¼ ì§•ê³„í•´ê³  í•˜ì˜€ë‹¤ë©´, ì´ëŠ” ì›ê³ ì˜ ë…¸ë™ì¡°í•© í™œë™ê³¼ëŠ” ê´€ê³„ì—†ì´ íšŒì‚¬ì·¨ì—…ê·œì¹™ì— ì˜í•˜ì—¬ ì‚¬ë‚´ì§ˆì„œë¥¼ ìœ ì§€í•˜ê¸° ìœ„í•œ ì‚¬ìš©ì ê³ ìœ ì˜ ì§•ê³„ê¶Œì— ê¸°í•˜ì—¬ ì´ë£¨ì–´ì§„ ì •ë‹¹í•œ ì§•ê³„ê¶Œì˜ í–‰ì‚¬ë¡œ ë³´ì•„ì•¼ í•œë‹¤.\\n\\n### ìš”ì•½:', 'target_text': 'ì›ê³ ê°€  ì£¼ë™í•˜ì—¬ íšŒì‚¬ì—…ë¬´ëŠ¥ë¥ ì„ ì €í•´í•˜ê³  íšŒì‚¬ì—…ë¬´ìƒì˜ ì§€íœ˜ëª…ë ¹ì— ìœ„ë°˜í•˜ì˜€ë‹¤ë©´ ì´ì— ë”°ë¥¸ ì§•ê³„í•´ê³ ëŠ” ì‚¬ë‚´ì§ˆì„œë¥¼ ìœ ì§€í•˜ê¸° ìœ„í•œ ì‚¬ìš©ì ê³ ìœ ì˜ ì •ë‹¹í•œ ì§•ê³„ê¶Œì˜ í–‰ì‚¬ë¡œ ë³´ì•„ì•¼ í•œë‹¤.'}\n"
     ]
    }
   ],
   "source": [
    "# 4. ì „ì²˜ë¦¬ í•¨ìˆ˜ ì •ì˜ (batched=True)\n",
    "def preprocess_function(examples):\n",
    "    \"\"\"\n",
    "    ê° ì˜ˆì œì— ëŒ€í•´ í”„ë¡¬í”„íŠ¸ì™€ íƒ€ê²Ÿ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "    - 'input_text' ì»¬ëŸ¼ì— ì €ì¥ëœ ì›ë¬¸ ì•ì— \"### ì›ë¬¸:\"ê³¼ \"### ìš”ì•½:\" êµ¬ë¶„ìë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.\n",
    "    - 'target_text' ì»¬ëŸ¼ì€ ê·¸ëŒ€ë¡œ ìš”ì•½ë¬¸ìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    prompts = []\n",
    "    targets = []\n",
    "    for document, summary in zip(examples[\"input_text\"], examples[\"target_text\"]):\n",
    "        prompt = \"### ì›ë¬¸:\\n\" + document + \"\\n\\n### ìš”ì•½:\"\n",
    "        prompts.append(prompt)\n",
    "        targets.append(summary)\n",
    "    return {\"input_text\": prompts, \"target_text\": targets}\n",
    "\n",
    "# 5. ë°ì´í„°ì…‹ì— ì „ì²˜ë¦¬ í•¨ìˆ˜ ì ìš©\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True)\n",
    "print(\"\\nTokenized Dataset Sample:\")\n",
    "print(tokenized_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ë°ì´í„° ì •ë³´ í™•ì¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë°ì´í„° ìƒ˜í”Œ:\n",
      "                                       original_text  \\\n",
      "0  ì›ê³ ê°€ ì†Œì†íšŒì‚¬ì˜ ë…¸ë™ì¡°í•©ì—ì„œ ë¶„ê·œê°€ ë°œìƒí•˜ì ë…¸ì¡°í™œë™ì„ êµ¬ì‹¤ë¡œ ì •ìƒì ì¸ ê·¼ë¬´ë¥¼ í•´...   \n",
      "1  ìˆ˜ì¶œì…ì—…ì²´ì¸ ì›ê³ ê°€ ì˜ë¥˜ì œí’ˆì„ ì œì¡°ã†ìˆ˜ì¶œí•¨ì— ìˆì–´ ê°™ì€ ê·¸ë£¹ë‚´ ì¢…í•©ë¬´ì—­ìƒì‚¬ì¸ ì†Œì™¸ ...   \n",
      "2  ê°€ë“±ê¸°ë‹´ë³´ê¶Œìê°€ ì œì†Œì „ í™”í•´ì¡°í•­ì— ë”°ë¼ ìê¸° ëª…ì˜ë¡œ ì†Œìœ ê¶Œì´ì „ì˜ ë³¸ë“±ê¸°ë¥¼ ê²½ë£Œí•œ í›„...   \n",
      "3  ê°€. ë¶€ê°€ê°€ì¹˜ì„¸ë²• ì œ22ì¡° ì œ3í•­ ë‹¨ì„œì— ì œ1í˜¸ì™€ ì œ2í˜¸ê°€ ë™ì‹œì— í•´ë‹¹í•œë‹¤ëŠ” ëœ»ì€ ...   \n",
      "4  ì†Œë“ì„¸ë²• ì œ116ì¡° ì œ1í•­ì˜ ê·œì •ì— ì˜í•˜ë©´ ì •ë¶€ëŠ” ê³¼ì„¸í‘œì¤€í™•ì •ì‹ ê³ ë¥¼ í•˜ì—¬ì•¼ í•  ìì—...   \n",
      "\n",
      "                                        summary_text  \n",
      "0  ì›ê³ ê°€  ì£¼ë™í•˜ì—¬ íšŒì‚¬ì—…ë¬´ëŠ¥ë¥ ì„ ì €í•´í•˜ê³  íšŒì‚¬ì—…ë¬´ìƒì˜ ì§€íœ˜ëª…ë ¹ì— ìœ„ë°˜í•˜ì˜€ë‹¤ë©´ ì´ì— ...  \n",
      "1  ìˆ˜ì¶œì…ì—…ì²´ì¸ ì›ê³ ê°€ ì˜ë¥˜ì œí’ˆì„ ì œì¡°ã†ìˆ˜ì¶œí•¨ì— ìˆì–´ ì†Œì™¸ íšŒì‚¬ì˜ ì§ìˆ˜ì¶œì‹¤ì ì„ ì§€ì›í•˜ê¸°...  \n",
      "2  ê°€ë“±ê¸°ë‹´ë³´ê¶Œìê°€ ì œì†Œì „ í™”í•´ì¡°í•­ì— ì˜í•´ ìê¸° ëª…ì˜ë¡œ ì†Œìœ ê¶Œì´ì „ì˜ ë³¸ë“±ê¸°ë¥¼ ê²½ë£Œí•˜ê³  ...  \n",
      "3  ë¶€ê°€ê°€ì¹˜ì„¸ë²• ì œ22ì¡° ì œ3í•­ ë‹¨ì„œì— ì œ1í˜¸ì™€ ì œ2í˜¸ê°€ ë™ì‹œì— í•´ë‹¹í•œë‹¤ëŠ” ì˜ë¯¸ëŠ” ì œ1...  \n",
      "4  ì†Œë“ì„¸ë²• ì œ116ì¡° ì œ1í•­ì— ë”°ë¥´ë©´ ì •ë¶€ëŠ” ê³¼ì„¸í‘œì¤€í™•ì •ì‹ ê³ ë¥¼ í•´ì•¼ í•  ìì— ëŒ€í•´ ë‹¹...  \n",
      "\n",
      "ë°ì´í„° ì •ë³´:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 24329 entries, 0 to 24328\n",
      "Data columns (total 2 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   original_text  24329 non-null  object\n",
      " 1   summary_text   24329 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 380.3+ KB\n",
      "None\n",
      "\n",
      "ìˆ«ìí˜• ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤. í•´ë‹¹ ì‹œê°í™”ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# CSV íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸° (íŒŒì¼ ê²½ë¡œë¥¼ ì‹¤ì œ ê²½ë¡œë¡œ ìˆ˜ì •í•˜ì„¸ìš”)\n",
    "csv_path = \"extracted_documents.csv\"\n",
    "df = pd.read_csv(csv_path, encoding=\"utf-8\")\n",
    "\n",
    "# ë°ì´í„°í”„ë ˆì„ êµ¬ì¡°ì™€ ì»¬ëŸ¼ ì •ë³´ ì¶œë ¥\n",
    "print(\"ë°ì´í„° ìƒ˜í”Œ:\")\n",
    "print(df.head())\n",
    "print(\"\\në°ì´í„° ì •ë³´:\")\n",
    "print(df.info())\n",
    "\n",
    "# ìˆ«ìí˜• ì»¬ëŸ¼ ì¶”ì¶œ\n",
    "numeric_cols = df.select_dtypes(include=[\"int64\", \"float64\"]).columns\n",
    "\n",
    "# ìˆ«ìí˜• ì»¬ëŸ¼ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸\n",
    "if len(numeric_cols) == 0:\n",
    "    print(\"\\nìˆ«ìí˜• ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤. í•´ë‹¹ ì‹œê°í™”ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.\")\n",
    "else:\n",
    "    # 1. ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ ë¶„í¬: íˆìŠ¤í† ê·¸ë¨ì„ í†µí•´ ê° ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ì˜ ë¶„í¬ë¥¼ íŒŒì•…\n",
    "    df[numeric_cols].hist(figsize=(12, 10), bins=20)\n",
    "    plt.suptitle(\"ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ íˆìŠ¤í† ê·¸ë¨\")\n",
    "    plt.show()\n",
    "\n",
    "    # 2. ì´ìƒì¹˜ íƒì§€ë¥¼ ìœ„í•œ ë°•ìŠ¤í”Œë¡¯: ê° ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ì˜ ë¶„í¬ì™€ ì´ìƒì¹˜ë¥¼ í™•ì¸\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    for i, col in enumerate(numeric_cols, 1):\n",
    "        plt.subplot(1, len(numeric_cols), i)\n",
    "        sns.boxplot(x=df[col])\n",
    "        plt.title(f\"{col} ë°•ìŠ¤í”Œë¡¯\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # 3. ë°€ë„ í”Œë¡¯: ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ì˜ ë¶„í¬ë¥¼ ë¶€ë“œëŸ¬ìš´ ê³¡ì„ ìœ¼ë¡œ ì‹œê°í™”\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for col in numeric_cols:\n",
    "        sns.kdeplot(df[col].dropna(), shade=True, label=col)\n",
    "    plt.title(\"ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ ë°€ë„ í”Œë¡¯\")\n",
    "    plt.xlabel(\"ê°’\")\n",
    "    plt.ylabel(\"ë°€ë„\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## â­3. ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ê°€ì ¸ì˜¤ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wanted-1/miniconda3/envs/whisper/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:05<00:00,  1.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer loaded with 4-bit quantization and LoRA applied.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "def load_model_and_tokenizer(model_name=\"google/gemma-7b-it\"):\n",
    "    \"\"\"\n",
    "    4ë¹„íŠ¸ ì–‘ìí™”ì™€ PEFT(LoRA)ë¥¼ ì ìš©í•˜ì—¬ ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.\n",
    "    4ë¹„íŠ¸ ì–‘ìí™” ì„¤ì •ì€ BitsAndBytesConfigì—ì„œ load_in_4bit=Trueë¥¼ ì‚¬ìš©í•˜ë©°,\n",
    "    ì¶”ê°€ ì„¤ì •ìœ¼ë¡œ ì—°ì‚° dtype, ì´ì¤‘ ì–‘ìí™” ì‚¬ìš© ì—¬ë¶€, ì–‘ìí™” íƒ€ì…ì„ ì§€ì •í•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    # 4-bit ì–‘ìí™” ì„¤ì •\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_compute_dtype=torch.float16,  # ì—°ì‚° ì‹œ ì‚¬ìš©í•  ë°ì´í„° íƒ€ì…\n",
    "        bnb_4bit_use_double_quant=True,         # ì´ì¤‘ ì–‘ìí™” ì‚¬ìš© (ë©”ëª¨ë¦¬ íš¨ìœ¨ ê°œì„ )\n",
    "        bnb_4bit_quant_type=\"nf4\"               # ì–‘ìí™” íƒ€ì…: \"nf4\" ë˜ëŠ” \"fp4\" ì¤‘ ì„ íƒ (ì¼ë°˜ì ìœ¼ë¡œ \"nf4\" ì¶”ì²œ)\n",
    "    )\n",
    "    # device_map = \"auto\"\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    # í† í¬ë‚˜ì´ì €ì™€ ì–‘ìí™”ëœ ëª¨ë¸ ë¡œë“œ\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=bnb_config,\n",
    "        #device_map=device_map\n",
    "    )\n",
    "    \n",
    "    # 4-bit ì–‘ìí™”ëœ ëª¨ë¸ì€ ê¸°ë³¸ì ìœ¼ë¡œ íŒŒì¸íŠœë‹ì´ ë¶ˆê°€ëŠ¥í•˜ë¯€ë¡œ, LoRA ì–´ëŒ‘í„°ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.\n",
    "    lora_config = LoraConfig(\n",
    "        r=8,\n",
    "        lora_alpha=32,\n",
    "        target_modules=[\"q_proj\", \"v_proj\"],  # ëª¨ë¸ êµ¬ì¡°ì— ë§ê²Œ ì¡°ì • (ì˜ˆ: Gemma ëª¨ë¸ì˜ ê²½ìš°)\n",
    "        lora_dropout=0.1,\n",
    "        bias=\"none\",\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "    )\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "# ì‚¬ìš© ì˜ˆì‹œ:\n",
    "model, tokenizer = load_model_and_tokenizer(\"google/gemma-7b-it\")\n",
    "print(\"Model and tokenizer loaded with 4-bit quantization and LoRA applied.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## â­4. ëª¨ë¸ íŠœë‹í•˜ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM, \n",
    "    BitsAndBytesConfig, \n",
    "    TrainingArguments, \n",
    "    Trainer, \n",
    "    DataCollatorForSeq2Seq\n",
    ")\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "def load_model_and_tokenizer(model_name=\"google/gemma-7b-it\"):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "# def load_model_and_tokenizer(model_name=\"google/gemma-7b-it\"):\n",
    "#     \"\"\"\n",
    "#     BitsAndBytesConfigë¥¼ ì‚¬ìš©í•˜ì—¬ 8-bit quantizationê³¼ CPU offloadë¥¼ ì ìš©í•´ ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤.\n",
    "#     \"\"\"\n",
    "#     bnb_config = BitsAndBytesConfig(\n",
    "#         load_in_8bit=True,           \n",
    "#         llm_int8_threshold=6.0,\n",
    "#         llm_int8_has_fp16_weight=False,\n",
    "#         llm_int8_enable_fp32_cpu_offload=True  # CPU offload í™œì„±í™”\n",
    "#     )\n",
    "    \n",
    "#     # device_mapì€ \"auto\"ë¥¼ ì‚¬ìš©í•˜ì—¬ ë‚´ë¶€ì ìœ¼ë¡œ offload ì „ëµì´ ì ìš©ë˜ë„ë¡ í•¨\n",
    "#     device_map = \"auto\"\n",
    "    \n",
    "#     tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "#     model = AutoModelForCausalLM.from_pretrained(\n",
    "#         model_name,\n",
    "#         quantization_config=bnb_config,\n",
    "#         device_map=device_map\n",
    "#     )\n",
    "#     return model, tokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### â­2) ë°ì´í„°ì…‹ ì¤€ë¹„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(csv_path, encoding=\"utf-8\"):\n",
    "    \"\"\"\n",
    "    CSV íŒŒì¼ì„ ë¶ˆëŸ¬ì™€ì„œ ë°ì´í„°ì…‹ì„ êµ¬ì¶•í•©ë‹ˆë‹¤.\n",
    "    CSV íŒŒì¼ì—ëŠ” 'original_text'ì™€ 'summary_text' ì»¬ëŸ¼ì´ ìˆë‹¤ê³  ê°€ì •í•˜ë©°,\n",
    "    ì´ë¥¼ 'input_text'ì™€ 'target_text'ë¡œ ì¬ì •ì˜í•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    from datasets import Dataset\n",
    "    \n",
    "    df = pd.read_csv(csv_path, encoding=encoding)\n",
    "    df = df.rename(columns={\"original_text\": \"input_text\", \"summary_text\": \"target_text\"})\n",
    "    dataset = Dataset.from_pandas(df)\n",
    "    return dataset\n",
    "\n",
    "def tokenize_dataset(dataset, tokenizer, max_input_length=1024, max_target_length=256):\n",
    "    \"\"\"\n",
    "    ë°ì´í„°ì…‹ì˜ ê° ì˜ˆì œì— ëŒ€í•´ ì…ë ¥ê³¼ íƒ€ê²Ÿ í…ìŠ¤íŠ¸ë¥¼ í† í°í™”í•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    def tokenize_function(example):\n",
    "        model_inputs = tokenizer(example[\"input_text\"], max_length=max_input_length, truncation=True)\n",
    "        with tokenizer.as_target_tokenizer():\n",
    "            labels = tokenizer(example[\"target_text\"], max_length=max_target_length, truncation=True)\n",
    "        model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "        return model_inputs\n",
    "    \n",
    "    tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=dataset.column_names)\n",
    "    return tokenized_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### â­3) í•™ìŠµ ì„¤ì • ë° ì‹¤í–‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, tokenizer, tokenized_dataset, output_dir=\"./results\", epochs=3, seed=42):\n",
    "    \"\"\"\n",
    "    í† í°í™”ëœ ë°ì´í„°ì…‹ì„ í•™ìŠµìš©ê³¼ í‰ê°€ìš©ìœ¼ë¡œ ë¶„í• í•˜ê³ , Trainerë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ fine-tuningí•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    # ë°ì´í„°ì…‹ ë¶„í• \n",
    "    split_dataset = tokenized_dataset.train_test_split(test_size=0.1, seed=seed)\n",
    "    train_dataset = split_dataset[\"train\"]\n",
    "    eval_dataset = split_dataset[\"test\"]\n",
    "    \n",
    "    # Data Collator ì„¤ì •\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "    \n",
    "    # TrainingArguments ì„¤ì •\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        learning_rate=5e-5,\n",
    "        num_train_epochs=epochs,\n",
    "        weight_decay=0.01,\n",
    "        fp16=True,\n",
    "        logging_steps=50,\n",
    "        save_total_limit=3,\n",
    "        seed=seed\n",
    "    )\n",
    "    \n",
    "    # Trainer ê°ì²´ ìƒì„±\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator\n",
    "    )\n",
    "    \n",
    "    # í•™ìŠµ ì‹¤í–‰\n",
    "    trainer.train()\n",
    "    \n",
    "    # í•™ìŠµ ì™„ë£Œ í›„ ëª¨ë¸ ì €ì¥\n",
    "    trainer.save_model(os.path.join(output_dir, \"gemma_finetuned_final\"))\n",
    "    \n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### â­4) ë©”ì¸ í•¨ìˆ˜: ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:05<00:00,  1.48s/it]\n",
      "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]/home/wanted-1/miniconda3/envs/whisper/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3970: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:00<00:00, 9289.42 examples/s]\n",
      "/home/wanted-1/miniconda3/envs/whisper/lib/python3.10/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_2181768/3916027822.py:27: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 288.00 MiB. GPU 0 has a total capacity of 23.63 GiB of which 265.00 MiB is free. Including non-PyTorch memory, this process has 23.24 GiB memory in use. Of the allocated memory 22.78 GiB is allocated by PyTorch, and 26.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m     train_model(model, tokenizer, tokenized_dataset, output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./results\u001b[39m\u001b[38;5;124m\"\u001b[39m, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 19\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 16\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m tokenized_dataset \u001b[38;5;241m=\u001b[39m tokenize_dataset(raw_dataset, tokenizer)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# 3. ëª¨ë¸ íŠœë‹\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenized_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./results\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 27\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, tokenizer, tokenized_dataset, output_dir, epochs, seed)\u001b[0m\n\u001b[1;32m     14\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[1;32m     15\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39moutput_dir,\n\u001b[1;32m     16\u001b[0m     evaluation_strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     23\u001b[0m     seed\u001b[38;5;241m=\u001b[39mseed\n\u001b[1;32m     24\u001b[0m )\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Trainer ê°ì²´ ìƒì„±\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_collator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_collator\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# í•™ìŠµ ì‹¤í–‰\u001b[39;00m\n\u001b[1;32m     37\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[0;32m~/miniconda3/envs/whisper/lib/python3.10/site-packages/transformers/utils/deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/whisper/lib/python3.10/site-packages/transformers/trainer.py:612\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, processing_class, model_init, compute_loss_func, compute_metrics, callbacks, optimizers, optimizer_cls_and_kwargs, preprocess_logits_for_metrics)\u001b[0m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;66;03m# Bnb Quantized models doesn't support `.to` operation.\u001b[39;00m\n\u001b[1;32m    608\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    609\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mplace_model_on_device\n\u001b[1;32m    610\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantization_method\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m QuantizationMethod\u001b[38;5;241m.\u001b[39mBITS_AND_BYTES\n\u001b[1;32m    611\u001b[0m ):\n\u001b[0;32m--> 612\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_move_model_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    614\u001b[0m \u001b[38;5;66;03m# Force n_gpu to 1 to avoid DataParallel as MP will manage the GPUs\u001b[39;00m\n\u001b[1;32m    615\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_model_parallel:\n",
      "File \u001b[0;32m~/miniconda3/envs/whisper/lib/python3.10/site-packages/transformers/trainer.py:899\u001b[0m, in \u001b[0;36mTrainer._move_model_to_device\u001b[0;34m(self, model, device)\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_move_model_to_device\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, device):\n\u001b[0;32m--> 899\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    900\u001b[0m     \u001b[38;5;66;03m# Moving a model to an XLA device disconnects the tied weights, so we have to retie them.\u001b[39;00m\n\u001b[1;32m    901\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mparallel_mode \u001b[38;5;241m==\u001b[39m ParallelMode\u001b[38;5;241m.\u001b[39mTPU \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtie_weights\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/miniconda3/envs/whisper/lib/python3.10/site-packages/transformers/modeling_utils.py:3162\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3157\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[1;32m   3158\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3159\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3160\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3161\u001b[0m         )\n\u001b[0;32m-> 3162\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/whisper/lib/python3.10/site-packages/torch/nn/modules/module.py:1343\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1340\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1341\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1343\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/whisper/lib/python3.10/site-packages/torch/nn/modules/module.py:903\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    902\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 903\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    905\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    906\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    907\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    908\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    913\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    914\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/whisper/lib/python3.10/site-packages/torch/nn/modules/module.py:903\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    902\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 903\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    905\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    906\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    907\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    908\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    913\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    914\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 903 (2 times)]\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/whisper/lib/python3.10/site-packages/torch/nn/modules/module.py:903\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    902\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 903\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    905\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    906\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    907\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    908\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    913\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    914\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/whisper/lib/python3.10/site-packages/torch/nn/modules/module.py:930\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    926\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    927\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    928\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    929\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 930\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    931\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    933\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/whisper/lib/python3.10/site-packages/torch/nn/modules/module.py:1329\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1323\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1324\u001b[0m             device,\n\u001b[1;32m   1325\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1326\u001b[0m             non_blocking,\n\u001b[1;32m   1327\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1328\u001b[0m         )\n\u001b[0;32m-> 1329\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1331\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1332\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1333\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1334\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1335\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 288.00 MiB. GPU 0 has a total capacity of 23.63 GiB of which 265.00 MiB is free. Including non-PyTorch memory, this process has 23.24 GiB memory in use. Of the allocated memory 22.78 GiB is allocated by PyTorch, and 26.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # 0. GPU ì„¤ì • (í•„ìš”í•œ ê²½ìš° í™˜ê²½ ë³€ìˆ˜ë‚˜ ì½”ë“œ ë‚´ device ì„¤ì •)\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Using device:\", device)\n",
    "    \n",
    "    # 1. ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ (ëª¨ë¸ì€ ì´ë¯¸ device_map=\"auto\"ë¡œ ì˜¬ë°”ë¥¸ ë””ë°”ì´ìŠ¤ì— ë¡œë“œë¨)\n",
    "    model, tokenizer = load_model_and_tokenizer(\"google/gemma-7b-it\")\n",
    "    # model.to(device)  # 8-bit ëª¨ë¸ì—ì„œëŠ” ì´ í˜¸ì¶œì´ í•„ìš”í•˜ì§€ ì•Šìœ¼ë©°, ì—ëŸ¬ê°€ ë°œìƒí•©ë‹ˆë‹¤.\n",
    "    \n",
    "    # 2. ë°ì´í„°ì…‹ ì¤€ë¹„ ë° í† í°í™”\n",
    "    csv_path = \"extracted_documents.csv\"  # CSV íŒŒì¼ ê²½ë¡œ ì§€ì •\n",
    "    raw_dataset = prepare_dataset(csv_path, encoding=\"utf-8\")\n",
    "    tokenized_dataset = tokenize_dataset(raw_dataset, tokenizer)\n",
    "    \n",
    "    # 3. ëª¨ë¸ íŠœë‹\n",
    "    train_model(model, tokenizer, tokenized_dataset, output_dir=\"./results\", epochs=3, seed=42)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m     train_model(model, tokenizer, tokenized_dataset, output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./results\u001b[39m\u001b[38;5;124m\"\u001b[39m, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 19\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmain\u001b[39m():\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# 0. GPU ì„¤ì • (í•„ìš”í•œ ê²½ìš° í™˜ê²½ ë³€ìˆ˜ë‚˜ ì½”ë“œ ë‚´ device ì„¤ì •)\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m     device \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing device:\u001b[39m\u001b[38;5;124m\"\u001b[39m, device)\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# 1. ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ (ëª¨ë¸ì€ ì´ë¯¸ device_map=\"auto\"ë¡œ ì˜¬ë°”ë¥¸ ë””ë°”ì´ìŠ¤ì— ë¡œë“œë¨)\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # 0. GPU ì„¤ì • (í•„ìš”í•œ ê²½ìš° í™˜ê²½ ë³€ìˆ˜ë‚˜ ì½”ë“œ ë‚´ device ì„¤ì •)\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Using device:\", device)\n",
    "    \n",
    "    # 1. ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ (ëª¨ë¸ì€ ì´ë¯¸ device_map=\"auto\"ë¡œ ì˜¬ë°”ë¥¸ ë””ë°”ì´ìŠ¤ì— ë¡œë“œë¨)\n",
    "    model, tokenizer = load_model_and_tokenizer(\"google/gemma-7b-it\")\n",
    "    # model.to(device)  # 8-bit ëª¨ë¸ì—ì„œëŠ” ì´ í˜¸ì¶œì´ í•„ìš”í•˜ì§€ ì•Šìœ¼ë©°, ì—ëŸ¬ê°€ ë°œìƒí•©ë‹ˆë‹¤.\n",
    "    \n",
    "    # 2. ë°ì´í„°ì…‹ ì¤€ë¹„ ë° í† í°í™”\n",
    "    csv_path = \"extracted_documents.csv\"  # CSV íŒŒì¼ ê²½ë¡œ ì§€ì •\n",
    "    raw_dataset = prepare_dataset(csv_path, encoding=\"utf-8\")\n",
    "    tokenized_dataset = tokenize_dataset(raw_dataset, tokenizer)\n",
    "    \n",
    "    # 3. ëª¨ë¸ íŠœë‹\n",
    "    train_model(model, tokenizer, tokenized_dataset, output_dir=\"./results\", epochs=3, seed=42)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. í”„ë¡¬í”„íŠ¸ ë° í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •í•˜ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 28\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# í”„ë¡¬í”„íŠ¸ ì˜ˆì‹œ (ê²½ì œ/ê¸ˆìœµ ì£¼ì œ)\u001b[39;00m\n\u001b[1;32m     22\u001b[0m sample_input \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m### ì›ë¬¸:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mìµœê·¼ í•œêµ­ ê²½ì œëŠ” ê¸€ë¡œë²Œ ê²½ê¸° ë‘”í™”ì™€ ë‚´ìˆ˜ ë¶€ì§„ ë“±ìœ¼ë¡œ ì–´ë ¤ì›€ì„ ê²ªê³  ìˆìœ¼ë©°, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mì •ë¶€ì™€ ê¸°ì—…ë“¤ì€ ë‹¤ì–‘í•œ ì •ì±…ê³¼ í˜ì‹ ì„ í†µí•´ ìœ„ê¸°ë¥¼ ê·¹ë³µí•˜ê³ ì ë…¸ë ¥í•˜ê³  ìˆìŠµë‹ˆë‹¤.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m### ìš”ì•½:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     27\u001b[0m )\n\u001b[0;32m---> 28\u001b[0m sample_summary \u001b[38;5;241m=\u001b[39m generate_summary(sample_input, \u001b[43mmodel\u001b[49m, tokenizer, max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m150\u001b[39m)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerated Summary:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, sample_summary)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# def generate_summary(input_text, model, tokenizer, max_new_tokens=150):\n",
    "#     \"\"\"\n",
    "#     Fine-tuningëœ ëª¨ë¸ì„ ì´ìš©í•´ í”„ë¡¬í”„íŠ¸ì— ë”°ë¥¸ ìš”ì•½ì„ ìƒì„±í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.\n",
    "#     ìƒì„± íŒŒë¼ë¯¸í„°(temperature, top_p ë“±)ëŠ” ì‹¤í—˜ ê²°ê³¼ì— ë”°ë¼ ì¡°ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "#     \"\"\"\n",
    "#     inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "#     with torch.no_grad():\n",
    "#         outputs = model.generate(\n",
    "#             input_ids=inputs[\"input_ids\"],\n",
    "#             attention_mask=inputs[\"attention_mask\"],\n",
    "#             max_new_tokens=max_new_tokens,\n",
    "#             do_sample=True,              # ë¬´ì‘ìœ„ì„± ìœ ì§€ (í•„ìš”ì— ë”°ë¼ ê²°ì •ì  ìƒì„±ë„ ê³ ë ¤)\n",
    "#             temperature=0.7,\n",
    "#             top_p=0.8,\n",
    "#             repetition_penalty=1.2,\n",
    "#             no_repeat_ngram_size=3,\n",
    "#             pad_token_id=tokenizer.pad_token_id or tokenizer.eos_token_id\n",
    "#         )\n",
    "#     return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# # í”„ë¡¬í”„íŠ¸ ì˜ˆì‹œ (ê²½ì œ/ê¸ˆìœµ ì£¼ì œ)\n",
    "# sample_input = (\n",
    "#     \"### ì›ë¬¸:\\n\"\n",
    "#     \"ìµœê·¼ í•œêµ­ ê²½ì œëŠ” ê¸€ë¡œë²Œ ê²½ê¸° ë‘”í™”ì™€ ë‚´ìˆ˜ ë¶€ì§„ ë“±ìœ¼ë¡œ ì–´ë ¤ì›€ì„ ê²ªê³  ìˆìœ¼ë©°, \"\n",
    "#     \"ì •ë¶€ì™€ ê¸°ì—…ë“¤ì€ ë‹¤ì–‘í•œ ì •ì±…ê³¼ í˜ì‹ ì„ í†µí•´ ìœ„ê¸°ë¥¼ ê·¹ë³µí•˜ê³ ì ë…¸ë ¥í•˜ê³  ìˆìŠµë‹ˆë‹¤.\\n\\n\"\n",
    "#     \"### ìš”ì•½:\"\n",
    "# )\n",
    "# sample_summary = generate_summary(sample_input, model, tokenizer, max_new_tokens=150)\n",
    "# print(\"Generated Summary:\\n\", sample_summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ì¶œë ¥ ê²°ê³¼ í™•ì¸ í›„ í”„ë¡¬í”„íŠ¸ ë° í•˜ì´í¼ íŒŒë¼ë¯¸í„° ì¡°ì •í•˜ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def evaluate_and_adjust():\n",
    "#     \"\"\"\n",
    "#     ìƒì„±ëœ ê²°ê³¼ë¥¼ í‰ê°€í•˜ê³ , í”„ë¡¬í”„íŠ¸ ë¬¸êµ¬ë‚˜ ìƒì„± íŒŒë¼ë¯¸í„°ë¥¼ ì¡°ì •í•˜ëŠ” ë°˜ë³µ ê³¼ì •ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.\n",
    "#     ì´ ë‹¨ê³„ëŠ” ìˆ˜ë™ ë˜ëŠ” ìë™í™”ëœ í‰ê°€ ë£¨í”„ë¥¼ í†µí•´ ì§„í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "#     \"\"\"\n",
    "#     # ì˜ˆì‹œ: ìƒì„± ê²°ê³¼ í‰ê°€ (ì‹¤ì œ í‰ê°€ ê¸°ì¤€ì€ ROUGE, BLEU ë“± ë©”íŠ¸ë¦­ì„ í™œìš©í•  ìˆ˜ ìˆìŒ)\n",
    "#     print(\"ì¶œë ¥ ê²°ê³¼ë¥¼ í™•ì¸í•˜ê³ , í”„ë¡¬í”„íŠ¸ ë° íŒŒë¼ë¯¸í„°ë¥¼ ì¡°ì •í•˜ì„¸ìš”.\")\n",
    "#     # í•„ìš” ì‹œ, ì¬ìƒì„± ë° íŒŒë¼ë¯¸í„° ë³€ê²½ ë¡œì§ì„ ì¶”ê°€í•©ë‹ˆë‹¤.\n",
    "\n",
    "# # í‰ê°€ ë° ì¡°ì • í˜¸ì¶œ ì˜ˆì‹œ\n",
    "# evaluate_and_adjust()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. íŠœë‹ ë£¨í”„ ë§Œë“¤ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tuning_loop(model, tokenizer, trainer, iterations=3):\n",
    "#     \"\"\"\n",
    "#     4~6ë‹¨ê³„ë¥¼ ë°˜ë³µí•˜ëŠ” íŠœë‹ ë£¨í”„ì…ë‹ˆë‹¤.\n",
    "#     iterations ê°’ì€ ì‹¤í—˜ì— ë”°ë¼ ì¡°ì •í•©ë‹ˆë‹¤.\n",
    "#     ê° ë°˜ë³µì—ì„œ ëª¨ë¸ì˜ ì¶œë ¥ ê²°ê³¼ë¥¼ í‰ê°€í•˜ê³ ,\n",
    "#     í•„ìš” ì‹œ í”„ë¡¬í”„íŠ¸ ë¬¸êµ¬ ë° í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì¡°ì •í•˜ì—¬ ìµœì ì˜ ê²°ê³¼ë¥¼ ë„ì¶œí•©ë‹ˆë‹¤.\n",
    "#     \"\"\"\n",
    "#     for i in range(iterations):\n",
    "#         print(f\"\\n=== Tuning iteration {i+1} ===\")\n",
    "#         # í”„ë¡¬í”„íŠ¸ ë° í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •\n",
    "#         sample_input = (\n",
    "#             \"### ì›ë¬¸:\\n\"\n",
    "#             \"ìµœê·¼ í•œêµ­ ê²½ì œëŠ” ê¸€ë¡œë²Œ ê²½ê¸° ë‘”í™”ì™€ ë‚´ìˆ˜ ë¶€ì§„ ë“±ìœ¼ë¡œ ì–´ë ¤ì›€ì„ ê²ªê³  ìˆìœ¼ë©°, \"\n",
    "#             \"ì •ë¶€ì™€ ê¸°ì—…ë“¤ì€ ë‹¤ì–‘í•œ ì •ì±…ê³¼ í˜ì‹ ì„ í†µí•´ ìœ„ê¸°ë¥¼ ê·¹ë³µí•˜ê³ ì ë…¸ë ¥í•˜ê³  ìˆìŠµë‹ˆë‹¤.\\n\\n\"\n",
    "#             \"### ìš”ì•½:\"\n",
    "#         )\n",
    "#         output = generate_summary(sample_input, model, tokenizer, max_new_tokens=150)\n",
    "#         print(\"Generated Output:\\n\", output)\n",
    "        \n",
    "#         # í‰ê°€ ë° í•„ìš” ì‹œ íŒŒë¼ë¯¸í„°, í”„ë¡¬í”„íŠ¸ ì¡°ì • (ì—¬ê¸°ì„œëŠ” ë‹¨ìˆœíˆ ì¶œë ¥ í™•ì¸)\n",
    "#         evaluate_and_adjust()\n",
    "    \n",
    "#     print(\"ìµœì¢… íŠœë‹ ì™„ë£Œ.\")\n",
    "\n",
    "# # íŠœë‹ ë£¨í”„ ì‹¤í–‰ ì˜ˆì‹œ\n",
    "# tuning_loop(model, tokenizer, trainer, iterations=3)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "whisper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
