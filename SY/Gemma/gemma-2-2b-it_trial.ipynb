{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import Dataset, load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline, TrainingArguments\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTTrainer\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import BitsAndBytesConfig\n",
    "# 원하는 GPU를 지정 (예: 두 번째 GPU 사용)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터셋"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최대 input_ids 길이: 3993\n",
      "최대 labels 길이: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Casting the dataset: 100%|██████████| 19463/19463 [00:00<00:00, 54621.90 examples/s]\n",
      "Casting the dataset: 100%|██████████| 2433/2433 [00:00<00:00, 53592.67 examples/s]\n",
      "Casting the dataset: 100%|██████████| 2433/2433 [00:00<00:00, 57573.89 examples/s]\n",
      "Casting the dataset: 100%|██████████| 19463/19463 [00:00<00:00, 814472.25 examples/s]\n",
      "Casting the dataset: 100%|██████████| 2433/2433 [00:00<00:00, 1330127.95 examples/s]\n",
      "Casting the dataset: 100%|██████████| 2433/2433 [00:00<00:00, 1358820.46 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'labels'],\n",
      "        num_rows: 19463\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'labels'],\n",
      "        num_rows: 2433\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'labels'],\n",
      "        num_rows: 2433\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "from datasets import Dataset, DatasetDict, Value, Features\n",
    "\n",
    "# 1. CSV 파일 불러오기\n",
    "df = pd.read_csv(\"extracted_documents.csv\")\n",
    "df = df.fillna(\"\")\n",
    "\n",
    "# 2. 필요한 컬럼만 추출하고 이름 변경\n",
    "df['input_ids'] = df['original_text']\n",
    "df['labels']  = df['summary_text']\n",
    "df = df[['input_ids', 'labels']]\n",
    "\n",
    "# 3. 문자열 길이 확인 (디버깅용)\n",
    "print(f\"최대 input_ids 길이: {df['input_ids'].str.len().max()}\")\n",
    "print(f\"최대 labels 길이: {df['labels'].str.len().max()}\")\n",
    "\n",
    "# 4. PyArrow Table로 변환 (large_string 적용)\n",
    "schema = pa.schema([\n",
    "    ('input_ids', pa.large_string()),\n",
    "    ('labels', pa.large_string())\n",
    "])\n",
    "table = pa.Table.from_pandas(df, schema=schema)\n",
    "\n",
    "\n",
    "\n",
    "# 5. Dataset으로 변환\n",
    "dataset = Dataset.from_dict({col: table[col].to_pandas() for col in table.schema.names})\n",
    "\n",
    "# 6. train/validation/test 데이터셋 분할 (8:1:1 비율 예시)\n",
    "#   - train_test_split() 함수를 두 번 호출해서\n",
    "#     먼저 train: 80%, test: 20% 분리하고,\n",
    "#     그 뒤 test 부분을 다시 50%씩 나눠 validation: 10%, test: 10% 로 분할\n",
    "split_dataset = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "temp_dataset  = split_dataset['test'].train_test_split(test_size=0.5, seed=42)\n",
    "\n",
    "final_dataset = DatasetDict({\n",
    "    'train': split_dataset['train'],\n",
    "    'validation': temp_dataset['train'],\n",
    "    'test': temp_dataset['test']\n",
    "})\n",
    "\n",
    "final_dataset = final_dataset.cast_column(\"input_ids\", Value(\"large_string\"))\n",
    "final_dataset = final_dataset.cast_column(\"labels\", Value(\"large_string\"))\n",
    "\n",
    "# # 스키마와 첫 번째 예시 확인\n",
    "# for split in final_dataset.keys():\n",
    "#     print(f\"[{split} 데이터셋] 스키마:\")\n",
    "#     print(final_dataset[split].features)\n",
    "#     print(f\"[{split} 데이터셋] 첫 번째 예시:\")\n",
    "#     print(final_dataset[split][0])\n",
    "# 7. 결과 확인\n",
    "print(final_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train 데이터셋] 스키마:\n",
      "{'document': Value(dtype='large_string', id=None), 'summary': Value(dtype='large_string', id=None)}\n",
      "[train 데이터셋] 첫 번째 예시:\n",
      "{'document': '[1] 분할보험료가 약정한 시기에 지급되지 아니한 경우 보험자는 상당한 기간을 정하여 보험계약자에게 최고하고 그 기간 안에 보험료가 지급되지 아니한 때에는 그 보험계약을 해지할 수 있으나, 보험계약자와 피보험자가 다른 때에는 상법 제650조 제3항에 따라 피보험자에게도 상당한 기간을 정하여 보험료의 지급을 최고한 뒤가 아니면 그 계약을 해지하지 못한다. [2] 보험계약자 또는 피보험자가 주소변경을 통보하지 아니하는 한 보험증권에 기재된 보험계약자 또는 피보험자의 주소를 보험회사의 의사표시를 수령할 지정장소로 본다는 개인용 자동차보험 특별약관의 규정은 보험회사가 과실 없이 보험계약자 또는 피보험자의 주소 등 소재를 알지 못한 경우에 한하여 적용된다.', 'summary': '약정한 시기에 분할보험료가 지급되지 않았다면, 보험자는 상당한 기간을 정해 보험계약자에게 최고하고 그 기간 안에 보험료가 지급되지 않은 경우 그 보험계약을 해지할 수 있으며, 개인용 자동차보험 특별약관의 규정에 따라 보험계약자 또는 피보험자가 주소변경을 통보하지 않는 것에 한하여 보험증권에 기재된 보험계약자 또는 피보험자의 주소를 보험회사의 의사표시를 수령할 지정장소로 보므로, 보험회사가 과실 없이 보험계약자 또는 피보험자의 소재를 알지 못한 경우에 한하여 적용된다.'}\n",
      "[validation 데이터셋] 스키마:\n",
      "{'document': Value(dtype='large_string', id=None), 'summary': Value(dtype='large_string', id=None)}\n",
      "[validation 데이터셋] 첫 번째 예시:\n",
      "{'document': '[1] 지입차주와 지입회사 사이의 구체적인 법률관계는 사적자치의 원칙에 따라 당사자 사이에 체결된 지입계약에 의하여 확정되는 것이므로 지입차주와 지입회사는 지입계약 내용에 따라 지입계약의 해지사유에 관하여 임의로 정할 수 있으며, 그와 같이 정하여진 지입계약의 해지사유가 명백하지 아니할 때에는 의사해석의 문제로 귀착되는 것으로서, 이러한 당사자의 의사를 해석할 때에는 지입계약이 체결된 동기 및 경위, 당사자가 계약을 통해 달성하려고 하는 목적과 진정한 의사 등을 종합적으로 고찰하여 사회정의와 형평의 이념에 맞도록 논리와 경험의 법칙 그리고 사회일반의 상식과 거래의 통념에 따라 합리적으로 해석하여야 한다. [2] 지입차주와 지입회사가 계약기간을 5년으로 정하여 지입계약을 체결하면서 지입차주에게는 계약기간 중의 임의해지를 명시적으로 인정하면서 위약금으로 일정 금액을 지입회사에 지급하도록 규정한 반면, 지입회사가 계약기간 중 임의로 해지할 수 있는지에 관하여는 명시적인 규정을 두지 않고, 다만 지입차주의 채무불이행이나 감차처분 등 일정한 해지사유가 있을 경우에 한하여 지입회사가 일방적으로 계약을 해지할 수 있도록 정한 사안에서, 지입회사는 계약기간 중에는 위 일정한 해지사유가 있을 경우에 한하여 계약을 해지할 수 있고 그러한 해지사유가 없을 경우에는 계약기간 중에 임의로 계약을 해지할 수 없다고 보는 것이 위 계약 조항에 대한 합리적 의사해석이라고 한 사례.', 'summary': '지입차주와 지입회사가 5년을 기간으로 하여 지입계약을 체결하면서 지입차주의 임의해지는 명시적으로 허용하고 지입회사의 임의해지에 대해서는 명시하지 않는다면 지입회사의 임의해지권은 인정되지 않는다.'}\n",
      "[test 데이터셋] 스키마:\n",
      "{'document': Value(dtype='large_string', id=None), 'summary': Value(dtype='large_string', id=None)}\n",
      "[test 데이터셋] 첫 번째 예시:\n",
      "{'document': '[1] 사실혼관계는 사실상의 관계를 기초로 하여 존재하는 것으로서 당사자 일방의 의사에 의하여 해소될 수 있고 당사자 일방의 파기로 인하여 공동생활의 사실이 없게 되면 사실상의 혼인관계는 해소되는 것이며, 다만 정당한 사유 없이 해소된 때에는 유책자가 상대방에 대하여 손해배상의 책임을 지는 데 지나지 않는다. [2] 사실혼관계의 당사자 중 일방이 의식불명이 된 상태에서 상대방이 사실혼관계의 해소를 주장하면서 재산분할심판청구를 한 사안에서, 위 사실혼관계는 상대방의 의사에 의하여 해소되었고 그에 따라 재산분할청구권이 인정된다고 본 사례.', 'summary': '당사자 일방의 의사에 의하여 사실혼은 해소될 수 있고 당사자 일방의 파기로 인하여 공동생활의 사실이 없게 된 경우 해소되는바 다만 정당한 사유 없이 해소된 때에는 유책자가 상대방에 대하여 손해배상의 책임을 지는 데 지나지 않기에 위 사안에서 사실혼관계는 상대방의 의사에 의하여 해소되었고 그에 따라 재산분할청구권이 인정된다.'}\n"
     ]
    }
   ],
   "source": [
    "# # 스키마와 첫 번째 예시 확인\n",
    "for split in final_dataset.keys():\n",
    "    print(f\"[{split} 데이터셋] 스키마:\")\n",
    "    print(final_dataset[split].features)\n",
    "    print(f\"[{split} 데이터셋] 첫 번째 예시:\")\n",
    "    print(final_dataset[split][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': '[1] 분할보험료가 약정한 시기에 지급되지 아니한 경우 보험자는 상당한 기간을 정하여 보험계약자에게 최고하고 그 기간 안에 보험료가 지급되지 아니한 때에는 그 보험계약을 해지할 수 있으나, 보험계약자와 피보험자가 다른 때에는 상법 제650조 제3항에 따라 피보험자에게도 상당한 기간을 정하여 보험료의 지급을 최고한 뒤가 아니면 그 계약을 해지하지 못한다. [2] 보험계약자 또는 피보험자가 주소변경을 통보하지 아니하는 한 보험증권에 기재된 보험계약자 또는 피보험자의 주소를 보험회사의 의사표시를 수령할 지정장소로 본다는 개인용 자동차보험 특별약관의 규정은 보험회사가 과실 없이 보험계약자 또는 피보험자의 주소 등 소재를 알지 못한 경우에 한하여 적용된다.',\n",
       " 'labels': '약정한 시기에 분할보험료가 지급되지 않았다면, 보험자는 상당한 기간을 정해 보험계약자에게 최고하고 그 기간 안에 보험료가 지급되지 않은 경우 그 보험계약을 해지할 수 있으며, 개인용 자동차보험 특별약관의 규정에 따라 보험계약자 또는 피보험자가 주소변경을 통보하지 않는 것에 한하여 보험증권에 기재된 보험계약자 또는 피보험자의 주소를 보험회사의 의사표시를 수령할 지정장소로 보므로, 보험회사가 과실 없이 보험계약자 또는 피보험자의 소재를 알지 못한 경우에 한하여 적용된다.'}"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_dataset['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BASE_MODEL = \"google/gemma-2b-it\"\n",
    "# model = AutoModelForCausalLM.from_pretrained(BASE_MODEL, device_map={\"\":0}).to(device)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc = final_dataset['train']['document'][0]\n",
    "# pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=512)\n",
    "# messages = [\n",
    "#     {\"role\": \"user\",\n",
    "#      \"content\": \"....\"}\n",
    "#      ]\n",
    "\n",
    "# prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gemma-it 추론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputs = pipe(\n",
    "#     prompt,\n",
    "#     do_sample=True,\n",
    "#     temperature=0.2,\n",
    "#     top_k=50,\n",
    "#     top_p=0.95,\n",
    "#     add_special_tokens=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 프롬프트 설정 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(example):\n",
    "    prompt_list = []\n",
    "    for i in range(len(example['document'])):\n",
    "        prompt_list.append(r\"\"\"<bos><start_of_turn>user\n",
    "다음 글을 요약해주세요:\n",
    "\n",
    "{}<end_of_turn>\n",
    "<start_of_turn>model\n",
    "{}<end_of_turn><eos>\"\"\".format(example['document'], example['summary']))\n",
    "    return prompt_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습-QLoRA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=6,\n",
    "    lora_alpha = 8,\n",
    "    lora_dropout = 0.05,\n",
    "    target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.60it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer\n",
    "from peft import LoraConfig, get_peft_model, PeftModel\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "\n",
    "BASE_MODEL = \"google/gemma-2b-it\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(BASE_MODEL, quantization_config=bnb_config)\n",
    "model = get_peft_model(model, lora_config) ##\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = 'right'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train 데이터셋] 스키마:\n",
      "{'document': Value(dtype='large_string', id=None), 'summary': Value(dtype='large_string', id=None)}\n",
      "[train 데이터셋] 첫 번째 예시:\n",
      "{'document': '[1] 분할보험료가 약정한 시기에 지급되지 아니한 경우 보험자는 상당한 기간을 정하여 보험계약자에게 최고하고 그 기간 안에 보험료가 지급되지 아니한 때에는 그 보험계약을 해지할 수 있으나, 보험계약자와 피보험자가 다른 때에는 상법 제650조 제3항에 따라 피보험자에게도 상당한 기간을 정하여 보험료의 지급을 최고한 뒤가 아니면 그 계약을 해지하지 못한다. [2] 보험계약자 또는 피보험자가 주소변경을 통보하지 아니하는 한 보험증권에 기재된 보험계약자 또는 피보험자의 주소를 보험회사의 의사표시를 수령할 지정장소로 본다는 개인용 자동차보험 특별약관의 규정은 보험회사가 과실 없이 보험계약자 또는 피보험자의 주소 등 소재를 알지 못한 경우에 한하여 적용된다.', 'summary': '약정한 시기에 분할보험료가 지급되지 않았다면, 보험자는 상당한 기간을 정해 보험계약자에게 최고하고 그 기간 안에 보험료가 지급되지 않은 경우 그 보험계약을 해지할 수 있으며, 개인용 자동차보험 특별약관의 규정에 따라 보험계약자 또는 피보험자가 주소변경을 통보하지 않는 것에 한하여 보험증권에 기재된 보험계약자 또는 피보험자의 주소를 보험회사의 의사표시를 수령할 지정장소로 보므로, 보험회사가 과실 없이 보험계약자 또는 피보험자의 소재를 알지 못한 경우에 한하여 적용된다.'}\n",
      "[validation 데이터셋] 스키마:\n",
      "{'document': Value(dtype='large_string', id=None), 'summary': Value(dtype='large_string', id=None)}\n",
      "[validation 데이터셋] 첫 번째 예시:\n",
      "{'document': '[1] 지입차주와 지입회사 사이의 구체적인 법률관계는 사적자치의 원칙에 따라 당사자 사이에 체결된 지입계약에 의하여 확정되는 것이므로 지입차주와 지입회사는 지입계약 내용에 따라 지입계약의 해지사유에 관하여 임의로 정할 수 있으며, 그와 같이 정하여진 지입계약의 해지사유가 명백하지 아니할 때에는 의사해석의 문제로 귀착되는 것으로서, 이러한 당사자의 의사를 해석할 때에는 지입계약이 체결된 동기 및 경위, 당사자가 계약을 통해 달성하려고 하는 목적과 진정한 의사 등을 종합적으로 고찰하여 사회정의와 형평의 이념에 맞도록 논리와 경험의 법칙 그리고 사회일반의 상식과 거래의 통념에 따라 합리적으로 해석하여야 한다. [2] 지입차주와 지입회사가 계약기간을 5년으로 정하여 지입계약을 체결하면서 지입차주에게는 계약기간 중의 임의해지를 명시적으로 인정하면서 위약금으로 일정 금액을 지입회사에 지급하도록 규정한 반면, 지입회사가 계약기간 중 임의로 해지할 수 있는지에 관하여는 명시적인 규정을 두지 않고, 다만 지입차주의 채무불이행이나 감차처분 등 일정한 해지사유가 있을 경우에 한하여 지입회사가 일방적으로 계약을 해지할 수 있도록 정한 사안에서, 지입회사는 계약기간 중에는 위 일정한 해지사유가 있을 경우에 한하여 계약을 해지할 수 있고 그러한 해지사유가 없을 경우에는 계약기간 중에 임의로 계약을 해지할 수 없다고 보는 것이 위 계약 조항에 대한 합리적 의사해석이라고 한 사례.', 'summary': '지입차주와 지입회사가 5년을 기간으로 하여 지입계약을 체결하면서 지입차주의 임의해지는 명시적으로 허용하고 지입회사의 임의해지에 대해서는 명시하지 않는다면 지입회사의 임의해지권은 인정되지 않는다.'}\n",
      "[test 데이터셋] 스키마:\n",
      "{'document': Value(dtype='large_string', id=None), 'summary': Value(dtype='large_string', id=None)}\n",
      "[test 데이터셋] 첫 번째 예시:\n",
      "{'document': '[1] 사실혼관계는 사실상의 관계를 기초로 하여 존재하는 것으로서 당사자 일방의 의사에 의하여 해소될 수 있고 당사자 일방의 파기로 인하여 공동생활의 사실이 없게 되면 사실상의 혼인관계는 해소되는 것이며, 다만 정당한 사유 없이 해소된 때에는 유책자가 상대방에 대하여 손해배상의 책임을 지는 데 지나지 않는다. [2] 사실혼관계의 당사자 중 일방이 의식불명이 된 상태에서 상대방이 사실혼관계의 해소를 주장하면서 재산분할심판청구를 한 사안에서, 위 사실혼관계는 상대방의 의사에 의하여 해소되었고 그에 따라 재산분할청구권이 인정된다고 본 사례.', 'summary': '당사자 일방의 의사에 의하여 사실혼은 해소될 수 있고 당사자 일방의 파기로 인하여 공동생활의 사실이 없게 된 경우 해소되는바 다만 정당한 사유 없이 해소된 때에는 유책자가 상대방에 대하여 손해배상의 책임을 지는 데 지나지 않기에 위 사안에서 사실혼관계는 상대방의 의사에 의하여 해소되었고 그에 따라 재산분할청구권이 인정된다.'}\n"
     ]
    }
   ],
   "source": [
    "# # 스키마와 첫 번째 예시 확인\n",
    "for split in final_dataset.keys():\n",
    "    print(f\"[{split} 데이터셋] 스키마:\")\n",
    "    print(final_dataset[split].features)\n",
    "    print(f\"[{split} 데이터셋] 첫 번째 예시:\")\n",
    "    print(final_dataset[split][0])\n",
    "    \n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': Value(dtype='large_string', id=None), 'labels': Value(dtype='large_string', id=None)}\n"
     ]
    }
   ],
   "source": [
    "print(final_dataset['test'].features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': Value(dtype='large_string', id=None),\n",
       " 'labels': Value(dtype='large_string', id=None)}"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data=final_dataset['test']\n",
    "train_data.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'document': Value(dtype='large_string', id=None), 'summary': Value(dtype='large_string', id=None)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Casting the dataset: 100%|██████████| 2433/2433 [00:00<00:00, 1246304.55 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# # 데이터셋의 모든 스키마 정보 확인\n",
    "# print(final_dataset['test'].features)\n",
    "\n",
    "# # 모든 문자열 필드를 large_string으로 변환할 새 Features 객체 생성\n",
    "# from datasets import Features, Value\n",
    "\n",
    "# # 모든 텍스트 필드를 large_string으로 변환\n",
    "# new_features = {}\n",
    "# for field_name, field_type in final_dataset['test'].features.items():\n",
    "#     if field_type.dtype == 'string':\n",
    "#         new_features[field_name] = Value(\"large_string\")\n",
    "#     else:\n",
    "#         new_features[field_name] = field_type\n",
    "\n",
    "# # 새 스키마로 데이터셋 변환\n",
    "# train_data = final_dataset['test'].cast(Features(new_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "import torch\n",
    "\n",
    "# ✅ 1. Trainer의 학습 파라미터 설정\n",
    "training_args = TrainingArguments(\n",
    "    remove_unused_columns=False,\n",
    "    output_dir=\"outputs\",\n",
    "    num_train_epochs=1,\n",
    "    max_steps=3,\n",
    "    per_device_train_batch_size=1,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    warmup_steps=10,\n",
    "    learning_rate=2e-5,\n",
    "    fp16=True,\n",
    "    logging_steps=10,\n",
    "    push_to_hub=False,\n",
    "    report_to='none',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can only join an iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[166], line 13\u001b[0m\n\u001b[1;32m      5\u001b[0m     train_data \u001b[38;5;241m=\u001b[39m Dataset\u001b[38;5;241m.\u001b[39mfrom_dict(train_data)\n\u001b[1;32m      7\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m      8\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m      9\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m     10\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtrain_data,\n\u001b[1;32m     11\u001b[0m )\n\u001b[0;32m---> 13\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/whisper/lib/python3.10/site-packages/transformers/trainer.py:2241\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2239\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2240\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2242\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2246\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/whisper/lib/python3.10/site-packages/transformers/trainer.py:2548\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2541\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2542\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[1;32m   2543\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2544\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[1;32m   2545\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[1;32m   2546\u001b[0m )\n\u001b[1;32m   2547\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m-> 2548\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2550\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2551\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2552\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2553\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2554\u001b[0m ):\n\u001b[1;32m   2555\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2556\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/miniconda3/envs/whisper/lib/python3.10/site-packages/transformers/trainer.py:3692\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3689\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mtrain):\n\u001b[1;32m   3690\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m-> 3692\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3693\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_sagemaker_mp_enabled():\n\u001b[1;32m   3694\u001b[0m     loss_mb \u001b[38;5;241m=\u001b[39m smp_forward_backward(model, inputs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps)\n",
      "File \u001b[0;32m~/miniconda3/envs/whisper/lib/python3.10/site-packages/transformers/trainer.py:3643\u001b[0m, in \u001b[0;36mTrainer._prepare_inputs\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3639\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_input(inputs)\n\u001b[1;32m   3640\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(inputs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   3641\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3642\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe batch received was empty, your model won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt be able to train on it. Double-check that your \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 3643\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining dataset contains keys expected by the model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_signature_columns\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3644\u001b[0m     )\n\u001b[1;32m   3645\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_past \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3646\u001b[0m     inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmems\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_past\n",
      "\u001b[0;31mTypeError\u001b[0m: can only join an iterable"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "if not isinstance(train_data, Dataset):\n",
    "    train_data = Dataset.from_dict(train_data)\n",
    "    \n",
    "trainer = Trainer(\n",
    "    model=model.to(\"cuda:0\"),\n",
    "    args=training_args,\n",
    "    train_dataset=train_data,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Applying formatting function to train dataset: 100%|██████████| 2433/2433 [00:21<00:00, 112.14 examples/s]\n",
      "Converting train dataset to ChatML:  82%|████████▏ | 1999/2433 [00:01<00:00, 1936.53 examples/s]\n"
     ]
    },
    {
     "ename": "ArrowInvalid",
     "evalue": "offset overflow while concatenating arrays, consider casting input from `string` to `large_string` first.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mArrowInvalid\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/whisper/lib/python3.10/site-packages/datasets/arrow_dataset.py:3511\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[0;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[1;32m   3510\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3511\u001b[0m         \u001b[43mwriter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3512\u001b[0m num_examples_progress_update \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/whisper/lib/python3.10/site-packages/datasets/arrow_writer.py:537\u001b[0m, in \u001b[0;36mArrowWriter.write\u001b[0;34m(self, example, key, writer_batch_size)\u001b[0m\n\u001b[1;32m    535\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhkey_record \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 537\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_examples_on_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/whisper/lib/python3.10/site-packages/datasets/arrow_writer.py:489\u001b[0m, in \u001b[0;36mArrowWriter.write_examples_on_file\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    484\u001b[0m     arrays \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    485\u001b[0m         chunk\n\u001b[1;32m    486\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m array \u001b[38;5;129;01min\u001b[39;00m arrays\n\u001b[1;32m    487\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m (array\u001b[38;5;241m.\u001b[39mchunks \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(array, pa\u001b[38;5;241m.\u001b[39mChunkedArray) \u001b[38;5;28;01melse\u001b[39;00m [array])\n\u001b[1;32m    488\u001b[0m     ]\n\u001b[0;32m--> 489\u001b[0m     batch_examples[col] \u001b[38;5;241m=\u001b[39m \u001b[43mpa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat_arrays\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/whisper/lib/python3.10/site-packages/pyarrow/array.pxi:4822\u001b[0m, in \u001b[0;36mpyarrow.lib.concat_arrays\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/whisper/lib/python3.10/site-packages/pyarrow/error.pxi:155\u001b[0m, in \u001b[0;36mpyarrow.lib.pyarrow_internal_check_status\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/whisper/lib/python3.10/site-packages/pyarrow/error.pxi:92\u001b[0m, in \u001b[0;36mpyarrow.lib.check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mArrowInvalid\u001b[0m: offset overflow while concatenating arrays, consider casting input from `string` to `large_string` first.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mArrowInvalid\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[114], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# train_data = final_dataset['test']\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# train_data = train_data.cast(Features({\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#     \"document\": Value(\"large_string\"),\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#     \"summary\": Value(\"large_string\")\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# }))\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mSFTTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;43;03m#    max_seq_length=512,\u001b[39;49;00m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTrainingArguments\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moutputs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_train_epochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[43mper_device_train_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;43;03m#        gradient_accumulation_steps=4,\u001b[39;49;00m\n\u001b[1;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpaged_adamw_8bit\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwarmup_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2e-5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfp16\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogging_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpush_to_hub\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreport_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnone\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m        \u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlora_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mformatting_func\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/whisper/lib/python3.10/site-packages/transformers/utils/deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/whisper/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:194\u001b[0m, in \u001b[0;36mSFTTrainer.__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, processing_class, compute_loss_func, compute_metrics, callbacks, optimizers, optimizer_cls_and_kwargs, preprocess_logits_for_metrics, peft_config, formatting_func)\u001b[0m\n\u001b[1;32m    192\u001b[0m preprocess_dataset \u001b[38;5;241m=\u001b[39m args\u001b[38;5;241m.\u001b[39mdataset_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args\u001b[38;5;241m.\u001b[39mdataset_kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip_prepare_dataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m preprocess_dataset:\n\u001b[0;32m--> 194\u001b[0m     train_dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocessing_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpacking\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformatting_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m    196\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m eval_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m         packing \u001b[38;5;241m=\u001b[39m args\u001b[38;5;241m.\u001b[39mpacking \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39meval_packing \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m args\u001b[38;5;241m.\u001b[39meval_packing\n",
      "File \u001b[0;32m~/miniconda3/envs/whisper/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:411\u001b[0m, in \u001b[0;36mSFTTrainer._prepare_dataset\u001b[0;34m(self, dataset, processing_class, args, packing, formatting_func, dataset_name)\u001b[0m\n\u001b[1;32m    409\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dataset, Dataset):  \u001b[38;5;66;03m# `IterableDataset.map` does not support `desc`\u001b[39;00m\n\u001b[1;32m    410\u001b[0m     map_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdesc\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConverting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m dataset to ChatML\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 411\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaybe_convert_to_chatml\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremove_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconversations\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconversations\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumn_names\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmap_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;66;03m# Apply the chat template if needed\u001b[39;00m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dataset, Dataset):  \u001b[38;5;66;03m# `IterableDataset.map` does not support `desc`\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/whisper/lib/python3.10/site-packages/datasets/arrow_dataset.py:562\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    555\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    556\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[1;32m    557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[1;32m    558\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[1;32m    559\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[1;32m    560\u001b[0m }\n\u001b[1;32m    561\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 562\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    563\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    564\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/whisper/lib/python3.10/site-packages/datasets/arrow_dataset.py:3079\u001b[0m, in \u001b[0;36mDataset.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   3073\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transformed_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3074\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m hf_tqdm(\n\u001b[1;32m   3075\u001b[0m         unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m examples\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3076\u001b[0m         total\u001b[38;5;241m=\u001b[39mpbar_total,\n\u001b[1;32m   3077\u001b[0m         desc\u001b[38;5;241m=\u001b[39mdesc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMap\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3078\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[0;32m-> 3079\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m rank, done, content \u001b[38;5;129;01min\u001b[39;00m Dataset\u001b[38;5;241m.\u001b[39m_map_single(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdataset_kwargs):\n\u001b[1;32m   3080\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[1;32m   3081\u001b[0m                 shards_done \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/whisper/lib/python3.10/site-packages/datasets/arrow_dataset.py:3546\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[0;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[1;32m   3544\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m update_data:\n\u001b[1;32m   3545\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m writer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 3546\u001b[0m         \u001b[43mwriter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfinalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3547\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tmp_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3548\u001b[0m         tmp_file\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/miniconda3/envs/whisper/lib/python3.10/site-packages/datasets/arrow_writer.py:636\u001b[0m, in \u001b[0;36mArrowWriter.finalize\u001b[0;34m(self, close_stream)\u001b[0m\n\u001b[1;32m    634\u001b[0m     \u001b[38;5;66;03m# Re-intializing to empty list for next batch\u001b[39;00m\n\u001b[1;32m    635\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhkey_record \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 636\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_examples_on_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;66;03m# If schema is known, infer features even if no examples were written\u001b[39;00m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpa_writer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mschema:\n",
      "File \u001b[0;32m~/miniconda3/envs/whisper/lib/python3.10/site-packages/datasets/arrow_writer.py:489\u001b[0m, in \u001b[0;36mArrowWriter.write_examples_on_file\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    483\u001b[0m     arrays \u001b[38;5;241m=\u001b[39m [row[\u001b[38;5;241m0\u001b[39m][col] \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_examples]\n\u001b[1;32m    484\u001b[0m     arrays \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    485\u001b[0m         chunk\n\u001b[1;32m    486\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m array \u001b[38;5;129;01min\u001b[39;00m arrays\n\u001b[1;32m    487\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m (array\u001b[38;5;241m.\u001b[39mchunks \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(array, pa\u001b[38;5;241m.\u001b[39mChunkedArray) \u001b[38;5;28;01melse\u001b[39;00m [array])\n\u001b[1;32m    488\u001b[0m     ]\n\u001b[0;32m--> 489\u001b[0m     batch_examples[col] \u001b[38;5;241m=\u001b[39m \u001b[43mpa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat_arrays\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    491\u001b[0m     batch_examples[col] \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    492\u001b[0m         row[\u001b[38;5;241m0\u001b[39m][col]\u001b[38;5;241m.\u001b[39mto_pylist()[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(row[\u001b[38;5;241m0\u001b[39m][col], (pa\u001b[38;5;241m.\u001b[39mArray, pa\u001b[38;5;241m.\u001b[39mChunkedArray)) \u001b[38;5;28;01melse\u001b[39;00m row[\u001b[38;5;241m0\u001b[39m][col]\n\u001b[1;32m    493\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_examples\n\u001b[1;32m    494\u001b[0m     ]\n",
      "File \u001b[0;32m~/miniconda3/envs/whisper/lib/python3.10/site-packages/pyarrow/array.pxi:4822\u001b[0m, in \u001b[0;36mpyarrow.lib.concat_arrays\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/whisper/lib/python3.10/site-packages/pyarrow/error.pxi:155\u001b[0m, in \u001b[0;36mpyarrow.lib.pyarrow_internal_check_status\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/whisper/lib/python3.10/site-packages/pyarrow/error.pxi:92\u001b[0m, in \u001b[0;36mpyarrow.lib.check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mArrowInvalid\u001b[0m: offset overflow while concatenating arrays, consider casting input from `string` to `large_string` first."
     ]
    }
   ],
   "source": [
    "# train_data = final_dataset['test']\n",
    "# train_data = train_data.cast(Features({\n",
    "#     \"document\": Value(\"large_string\"),\n",
    "#     \"summary\": Value(\"large_string\")\n",
    "# }))\n",
    "trainer = SFTTrainer(\n",
    "    model=model.to(device),\n",
    "    train_dataset=train_data,\n",
    "#    max_seq_length=512,\n",
    "    args=TrainingArguments(\n",
    "        output_dir=\"outputs\",\n",
    "        num_train_epochs = 1,\n",
    "        max_steps=3,\n",
    "        per_device_train_batch_size=1,\n",
    "#        gradient_accumulation_steps=4,\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        warmup_steps=10,\n",
    "        learning_rate=2e-5,\n",
    "        fp16=True,\n",
    "        logging_steps=10,\n",
    "        push_to_hub=False,\n",
    "        report_to='none',\n",
    "        \n",
    "    ),\n",
    "    peft_config=lora_config,\n",
    "    formatting_func=generate_prompt,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in replica 1 on device 1.\nOriginal Traceback (most recent call last):\n  File \"/home/wanted-1/miniconda3/envs/whisper/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 96, in _worker\n    output = module(*input, **kwargs)\n  File \"/home/wanted-1/miniconda3/envs/whisper/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/wanted-1/miniconda3/envs/whisper/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/wanted-1/miniconda3/envs/whisper/lib/python3.10/site-packages/peft/peft_model.py\", line 1719, in forward\n    return self.base_model(\n  File \"/home/wanted-1/miniconda3/envs/whisper/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/wanted-1/miniconda3/envs/whisper/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/wanted-1/miniconda3/envs/whisper/lib/python3.10/site-packages/peft/tuners/tuners_utils.py\", line 197, in forward\n    return self.model.forward(*args, **kwargs)\n  File \"/home/wanted-1/miniconda3/envs/whisper/lib/python3.10/site-packages/transformers/utils/deprecation.py\", line 172, in wrapped_func\n    return func(*args, **kwargs)\n  File \"/home/wanted-1/miniconda3/envs/whisper/lib/python3.10/site-packages/transformers/models/gemma/modeling_gemma.py\", line 844, in forward\n    outputs = self.model(\n  File \"/home/wanted-1/miniconda3/envs/whisper/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/wanted-1/miniconda3/envs/whisper/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/wanted-1/miniconda3/envs/whisper/lib/python3.10/site-packages/transformers/models/gemma/modeling_gemma.py\", line 597, in forward\n    layer_outputs = decoder_layer(\n  File \"/home/wanted-1/miniconda3/envs/whisper/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/wanted-1/miniconda3/envs/whisper/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/wanted-1/miniconda3/envs/whisper/lib/python3.10/site-packages/transformers/models/gemma/modeling_gemma.py\", line 332, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/home/wanted-1/miniconda3/envs/whisper/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/wanted-1/miniconda3/envs/whisper/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/wanted-1/miniconda3/envs/whisper/lib/python3.10/site-packages/transformers/models/gemma/modeling_gemma.py\", line 266, in forward\n    query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n  File \"/home/wanted-1/miniconda3/envs/whisper/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/wanted-1/miniconda3/envs/whisper/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/wanted-1/miniconda3/envs/whisper/lib/python3.10/site-packages/peft/tuners/lora/bnb.py\", line 496, in forward\n    result = self.base_layer(x, *args, **kwargs)\n  File \"/home/wanted-1/miniconda3/envs/whisper/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/wanted-1/miniconda3/envs/whisper/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/wanted-1/miniconda3/envs/whisper/lib/python3.10/site-packages/bitsandbytes/nn/modules.py\", line 484, in forward\n    return bnb.matmul_4bit(x, self.weight.t(), bias=bias, quant_state=self.weight.quant_state).to(inp_dtype)\n  File \"/home/wanted-1/miniconda3/envs/whisper/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py\", line 533, in matmul_4bit\n    return MatMul4Bit.apply(A, B, out, bias, quant_state)\n  File \"/home/wanted-1/miniconda3/envs/whisper/lib/python3.10/site-packages/torch/autograd/function.py\", line 575, in apply\n    return super().apply(*args, **kwargs)  # type: ignore[misc]\n  File \"/home/wanted-1/miniconda3/envs/whisper/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py\", line 462, in forward\n    output = torch.nn.functional.linear(A, F.dequantize_4bit(B, quant_state).to(A.dtype).t(), bias)\n  File \"/home/wanted-1/miniconda3/envs/whisper/lib/python3.10/site-packages/bitsandbytes/functional.py\", line 1363, in dequantize_4bit\n    is_on_gpu([A, absmax, out])\n  File \"/home/wanted-1/miniconda3/envs/whisper/lib/python3.10/site-packages/bitsandbytes/functional.py\", line 469, in is_on_gpu\n    raise RuntimeError(\nRuntimeError: Input tensors need to be on the same GPU, but found the following tensor and device combinations:\n [(torch.Size([1, 2097152]), device(type='cuda', index=1)), (torch.Size([65536]), device(type='cuda', index=0)), (torch.Size([2048, 2048]), device(type='cuda', index=1))]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[92], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/whisper/lib/python3.10/site-packages/transformers/trainer.py:2241\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2239\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2240\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2242\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2246\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/whisper/lib/python3.10/site-packages/transformers/trainer.py:2548\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2541\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2542\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[1;32m   2543\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2544\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[1;32m   2545\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[1;32m   2546\u001b[0m )\n\u001b[1;32m   2547\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m-> 2548\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2550\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2551\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2552\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2553\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2554\u001b[0m ):\n\u001b[1;32m   2555\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2556\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/miniconda3/envs/whisper/lib/python3.10/site-packages/transformers/trainer.py:3698\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3695\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   3697\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 3698\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3700\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[1;32m   3701\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   3702\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3703\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   3704\u001b[0m ):\n",
      "File \u001b[0;32m~/miniconda3/envs/whisper/lib/python3.10/site-packages/transformers/trainer.py:3759\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3757\u001b[0m         loss_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_items_in_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_items_in_batch\n\u001b[1;32m   3758\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mloss_kwargs}\n\u001b[0;32m-> 3759\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3760\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   3761\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   3762\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/whisper/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/whisper/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/whisper/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py:193\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule(\u001b[38;5;241m*\u001b[39minputs[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodule_kwargs[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    192\u001b[0m replicas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplicate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_ids[: \u001b[38;5;28mlen\u001b[39m(inputs)])\n\u001b[0;32m--> 193\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparallel_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather(outputs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_device)\n",
      "File \u001b[0;32m~/miniconda3/envs/whisper/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py:212\u001b[0m, in \u001b[0;36mDataParallel.parallel_apply\u001b[0;34m(self, replicas, inputs, kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mparallel_apply\u001b[39m(\n\u001b[1;32m    210\u001b[0m     \u001b[38;5;28mself\u001b[39m, replicas: Sequence[T], inputs: Sequence[Any], kwargs: Any\n\u001b[1;32m    211\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Any]:\n\u001b[0;32m--> 212\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparallel_apply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/whisper/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:126\u001b[0m, in \u001b[0;36mparallel_apply\u001b[0;34m(modules, inputs, kwargs_tup, devices)\u001b[0m\n\u001b[1;32m    124\u001b[0m     output \u001b[38;5;241m=\u001b[39m results[i]\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, ExceptionWrapper):\n\u001b[0;32m--> 126\u001b[0m         \u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    127\u001b[0m     outputs\u001b[38;5;241m.\u001b[39mappend(output)\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/miniconda3/envs/whisper/lib/python3.10/site-packages/torch/_utils.py:733\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    729\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    730\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    731\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    732\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 733\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in replica 1 on device 1.\nOriginal Traceback (most recent call last):\n  File \"/home/wanted-1/miniconda3/envs/whisper/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 96, in _worker\n    output = module(*input, **kwargs)\n  File \"/home/wanted-1/miniconda3/envs/whisper/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/wanted-1/miniconda3/envs/whisper/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/wanted-1/miniconda3/envs/whisper/lib/python3.10/site-packages/peft/peft_model.py\", line 1719, in forward\n    return self.base_model(\n  File \"/home/wanted-1/miniconda3/envs/whisper/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/wanted-1/miniconda3/envs/whisper/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/wanted-1/miniconda3/envs/whisper/lib/python3.10/site-packages/peft/tuners/tuners_utils.py\", line 197, in forward\n    return self.model.forward(*args, **kwargs)\n  File \"/home/wanted-1/miniconda3/envs/whisper/lib/python3.10/site-packages/transformers/utils/deprecation.py\", line 172, in wrapped_func\n    return func(*args, **kwargs)\n  File \"/home/wanted-1/miniconda3/envs/whisper/lib/python3.10/site-packages/transformers/models/gemma/modeling_gemma.py\", line 844, in forward\n    outputs = self.model(\n  File \"/home/wanted-1/miniconda3/envs/whisper/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/wanted-1/miniconda3/envs/whisper/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/wanted-1/miniconda3/envs/whisper/lib/python3.10/site-packages/transformers/models/gemma/modeling_gemma.py\", line 597, in forward\n    layer_outputs = decoder_layer(\n  File \"/home/wanted-1/miniconda3/envs/whisper/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/wanted-1/miniconda3/envs/whisper/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/wanted-1/miniconda3/envs/whisper/lib/python3.10/site-packages/transformers/models/gemma/modeling_gemma.py\", line 332, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/home/wanted-1/miniconda3/envs/whisper/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/wanted-1/miniconda3/envs/whisper/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/wanted-1/miniconda3/envs/whisper/lib/python3.10/site-packages/transformers/models/gemma/modeling_gemma.py\", line 266, in forward\n    query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n  File \"/home/wanted-1/miniconda3/envs/whisper/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/wanted-1/miniconda3/envs/whisper/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/wanted-1/miniconda3/envs/whisper/lib/python3.10/site-packages/peft/tuners/lora/bnb.py\", line 496, in forward\n    result = self.base_layer(x, *args, **kwargs)\n  File \"/home/wanted-1/miniconda3/envs/whisper/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/wanted-1/miniconda3/envs/whisper/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/wanted-1/miniconda3/envs/whisper/lib/python3.10/site-packages/bitsandbytes/nn/modules.py\", line 484, in forward\n    return bnb.matmul_4bit(x, self.weight.t(), bias=bias, quant_state=self.weight.quant_state).to(inp_dtype)\n  File \"/home/wanted-1/miniconda3/envs/whisper/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py\", line 533, in matmul_4bit\n    return MatMul4Bit.apply(A, B, out, bias, quant_state)\n  File \"/home/wanted-1/miniconda3/envs/whisper/lib/python3.10/site-packages/torch/autograd/function.py\", line 575, in apply\n    return super().apply(*args, **kwargs)  # type: ignore[misc]\n  File \"/home/wanted-1/miniconda3/envs/whisper/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py\", line 462, in forward\n    output = torch.nn.functional.linear(A, F.dequantize_4bit(B, quant_state).to(A.dtype).t(), bias)\n  File \"/home/wanted-1/miniconda3/envs/whisper/lib/python3.10/site-packages/bitsandbytes/functional.py\", line 1363, in dequantize_4bit\n    is_on_gpu([A, absmax, out])\n  File \"/home/wanted-1/miniconda3/envs/whisper/lib/python3.10/site-packages/bitsandbytes/functional.py\", line 469, in is_on_gpu\n    raise RuntimeError(\nRuntimeError: Input tensors need to be on the same GPU, but found the following tensor and device combinations:\n [(torch.Size([1, 2097152]), device(type='cuda', index=1)), (torch.Size([65536]), device(type='cuda', index=0)), (torch.Size([2048, 2048]), device(type='cuda', index=1))]\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ADAPTER_MODEL = \"lora_adapter\"\n",
    "\n",
    "trainer.model.save_pretrained(ADAPTER_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ls -alh lora_adapter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-LoRA 와 모델 합치기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.41it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'ADAPTER_MODEL' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(BASE_MODEL, device_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m, torch_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat16)\n\u001b[0;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m PeftModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(model, \u001b[43mADAPTER_MODEL\u001b[49m, device_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m, torch_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat16)\n\u001b[1;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mmerge_and_unload()\n\u001b[1;32m      5\u001b[0m model\u001b[38;5;241m.\u001b[39msave_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgemma-2b-it-sum-ko\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ADAPTER_MODEL' is not defined"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(BASE_MODEL, torch_dtype=torch.float16)\n",
    "model = PeftModel.from_pretrained(model, ADAPTER_MODEL, torch_dtype=torch.float16)\n",
    "\n",
    "model = model.merge_and_unload()\n",
    "model.save_pretrained('gemma-2b-it-sum-ko')\n",
    "\n",
    "# !ls -alh ./gemma-2b-it-sum-ko"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuned 모델 로드 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_MODEL = \"google/gemma-2b-it\"\n",
    "FINETUNE_MODEL = \"./gemma-2b-it-sum-ko\"\n",
    "\n",
    "finetune_model = AutoModelForCausalLM.from_pretrained(FINETUNE_MODEL, device_map={\"\":0})\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, add_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_finetuned = pipeline(\"text-generation\", model=finetune_model, tokenizer=tokenizer, max_new_tokens=512)\n",
    "doc = dataset['test']['document'][10]\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"다음 글을 요약해주세요:\\n\\n{}\".format(doc)\n",
    "    }\n",
    "]\n",
    "prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = pipe_finetuned(\n",
    "    prompt,\n",
    "    do_sample=True,\n",
    "    temperature=0.2,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    add_special_tokens=True\n",
    ")\n",
    "print(outputs[0][\"generated_text\"][len(prompt):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "whisper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
