{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/home/wanted-1/potenup-workspace/Project/project3/team2/SY/Gemma/processed_text.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wanted-1/miniconda3/envs/whisper/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import re\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import PeftModel, prepare_model_for_kbit_training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.79it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): GemmaForCausalLM(\n",
       "      (model): GemmaModel(\n",
       "        (embed_tokens): Embedding(256000, 2048, padding_idx=0)\n",
       "        (layers): ModuleList(\n",
       "          (0-17): 18 x GemmaDecoderLayer(\n",
       "            (self_attn): GemmaAttention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=256, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=256, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "            )\n",
       "            (mlp): GemmaMLP(\n",
       "              (gate_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=16384, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=16384, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=16384, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=16384, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=16384, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=16384, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): GELUActivation()\n",
       "            )\n",
       "            (input_layernorm): GemmaRMSNorm((2048,), eps=1e-06)\n",
       "            (post_attention_layernorm): GemmaRMSNorm((2048,), eps=1e-06)\n",
       "          )\n",
       "        )\n",
       "        (norm): GemmaRMSNorm((2048,), eps=1e-06)\n",
       "        (rotary_emb): GemmaRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=2048, out_features=256000, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. 설정: 베이스 모델 및 quantization 설정\n",
    "BASE_MODEL = \"google/gemma-2b-it\"\n",
    "\n",
    "#동일한 환경에서 모델과 LoRA 어댑터를 올바르게 불러오기 위해 필요\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# 2. 베이스 모델 불러오기 및 k-bit 학습 준비\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map={\"\": torch.cuda.current_device()}\n",
    ")\n",
    "base_model = prepare_model_for_kbit_training(base_model)\n",
    "\n",
    "# 3. 저장한 LoRA 어댑터 불러오기\n",
    "model = PeftModel.from_pretrained(base_model, \"path/to/save/lora_adapters_ver1_law\")\n",
    "model.eval()  # 평가 모드로 전환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. 토크나이저 초기화\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------\n",
    "# 1. 생성 하이퍼파라미터 조정\n",
    "generation_kwargs = {\n",
    "    \"max_length\": 4096,           # 최대 생성 토큰 수 (요약 길이 제한)\n",
    "    \"min_length\": 512,            # 최소 생성 토큰 수\n",
    "    \"do_sample\": True,           # 샘플링 방식 사용\n",
    "    \"temperature\": 0.5,          # 낮은 값은 결정론적, 높은 값은 다양성 증가\n",
    "    \"top_k\": 50,                 # 상위 k개 단어 내에서 샘플링\n",
    "    \"top_p\": 0.95,               # 누적 확률 p 내 단어에서 샘플링\n",
    "    \"num_return_sequences\": 1   # 3개의 후보 요약 생성 (후보 비교를 위해)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. 요약할 텍스트 파일 불러오기\n",
    "txt_file_path = \"/home/wanted-1/potenup-workspace/Project/project3/team2/SY/Gemma/processed_text.txt\"  # 파일 경로를 수정하세요.\n",
    "with open(txt_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    original_text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "make summary\n",
    "#  Write the summary in complete sentences, ensuring that each sentence ends with a period.\n",
    "# - The summary must be composed of at least 3 sentences and no more than 5 sentences.\n",
    "# - Avoid unnecessary elaboration or repetition; be concise and clear.\n",
    "# - Accurately reflect the core information of the original text.\n",
    "# - \"했으며\" 라는 표현 줄이고, 문장 단위로 작성\n",
    "# - Avoid using ','. \n",
    "# - Accurately reflect the core information of the original text.\n",
    "# - Do not use compound sentences with connectors like 'and' (minimize expressions similar to \"며,\"); write each sentence independently.\n",
    "# - Do not use commas.\n",
    "# - Make sure the summary is clear, concise, and written in separate sentences.\n",
    "# - Only output the summary.\n",
    "# - Do not include any notes, disclaimers, or additional comments.\n",
    "\n",
    "# After reading the original text below, please write a summary following these guidelines:\n",
    "# - Summarize in English first\n",
    "# - Make sure the summary is clear, concise, and written in separate sentences.\n",
    "# - The summary must be composed of at least 3 sentences and no more than 5 sentences.\n",
    "\n",
    "# - then translate it into Korean.\n",
    "# - Translate sentence by sentence, keeping each sentence distinct.\n",
    "# - Ensure that each translated sentence remains separate and does not merge.\n",
    "# - Do not write '**Note:**' AND 'Note:'\n",
    "# - Do not include any notes, disclaimers, or additional comments. Only output the summary.\n",
    "\n",
    "# - Avoid commas. Write short and clear sentences instead.\n",
    "# - Provide the summary as bullet points\n",
    "# - Each bullet point should include the main ideas, such as key events, people, dates, and any significant background.\n",
    "# - Include all the main ideas and key details of the original text.\n",
    "# - Do not use ','\n",
    "\n",
    "#  Write the summary in complete sentences, ensuring that each sentence ends with a period.\n",
    "# - The summary must be composed of at least 3 sentences and no more than 5 sentences.\n",
    "# - Avoid unnecessary elaboration or repetition; be concise and clear.\n",
    "# - Accurately reflect the core information of the original text.\n",
    "# - \"했으며\" 라는 표현 줄이고, 문장 단위로 작성\n",
    "# - Avoid using ','. \n",
    "# - Accurately reflect the core information of the original text.\n",
    "# - Do not use compound sentences with connectors like 'and' (minimize expressions similar to \"며,\"); write each sentence independently.\n",
    "# - Do not use commas.\n",
    "# - Make sure the summary is clear, concise, and written in separate sentences.\n",
    "# - Only output the summary.\n",
    "# - Do not include any notes, disclaimers, or additional comments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 상세 프롬프트 구성\n",
    "# 프롬프트에 구체적인 요약 작성 지시와 예시를 포함하여 모델이 어떤 정보를 포함해야 하는지 명확히 유도합니다.\n",
    "detailed_prompt = \"\"\"\n",
    "\n",
    "After reading the original text below, please perform the following steps in order:\n",
    "\n",
    "1. Translate the original text into English.\n",
    "2. Summarize the translated text by extracting only the most important core points. Present each key point as a numbered item (for example, \"1.\", \"2.\", \"3.\", etc.). Write each point concisely and clearly.\n",
    "3. Finally, translate the numbered summary into Korean.\n",
    "\n",
    "Important:\n",
    "- The final output must be only the summary in Korean.\n",
    "- Use simple and direct language.\n",
    "- Do not include any extra commentary, notes, or repeated information.\n",
    "- Each numbered point must be on a separate line and end with a period.\n",
    "- Do not include any bullet symbols (such as \"*\" or \"-\").\n",
    "- Do not write '**Note:**' AND 'Note:'\n",
    "- Do not include any notes, disclaimers, or additional comments. Only output the summary.\n",
    "\n",
    "Original Text:\n",
    "{original_text}\n",
    "\n",
    "Summary:\n",
    "\"\"\"\n",
    "\n",
    "# 실제 원문 텍스트 (예시로 직접 입력하거나 파일에서 읽어올 수 있음)\n",
    "#original_text = \"여기에 실제 원문 텍스트를 입력합니다. 이 텍스트는 예시로서 모델이 요약할 내용입니다.\"\n",
    "\n",
    "# 프롬프트 구성\n",
    "prompt = detailed_prompt.format(original_text=original_text)\n",
    "\n",
    "# 4. 입력 토큰화\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. 모델을 통한 요약 생성\n",
    "with torch.no_grad():\n",
    "    output_tokens = model.generate(**inputs, **generation_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------\n",
    "# 4. 후처리 함수 정의: 생성된 텍스트를 정제합니다.\n",
    "def post_process(text):\n",
    "    # 앞뒤 공백 제거\n",
    "    text = text.strip()\n",
    "    # 불필요한 공백, 줄바꿈, 중복 문장 부호 제거 (예시: 느낌표, 공백 등)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'!{2,}', '!', text)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Refined Summary:\n",
      "국정원이 북한 해커의 위장 취업을 조심하라 주의령을 내렸는데, 이는 국정원이 국내 한 에너지 기업의 해외지사에 취업하려던 북한 해커를 적발해 조사 중이라고 밝혔으며, 이 해커는 굳이 플랫폼 사이트에 등록하는 대담함도 보였으며, 국정원은 대남 사이버 테러를 주도했던 김영철의 복기도 주시하고 있으며, 채널A 뉴스 배두원이라고 한다.\n",
      "Note:\n",
      "국정원은 우리나라가 의제를 선호하듯 북한은 IT 분야 진출을 선호한다고도 했습니다.\n",
      "국정원은 대남 사이버 테러를 주도했던 김영철의 복기도 주시하고 있다.\n"
     ]
    }
   ],
   "source": [
    "generated_text = tokenizer.decode(output_tokens[0], skip_special_tokens=True)\n",
    "# \"요약:\" 이후의 텍스트가 실제 요약이라고 가정\n",
    "import re\n",
    "\n",
    "def refine_sentences(text):\n",
    "    \"\"\"\n",
    "    1. 문장을 적절히 분리하고 중복 제거\n",
    "    2. 잘못 추가된 마침표를 정리하여 자연스럽게 변환\n",
    "    \"\"\"\n",
    "    # 1️⃣ 문장 단위로 나누기 (마침표, 물음표, 느낌표 기준)\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', text.strip())\n",
    "\n",
    "    # 2️⃣ 중복 문장 제거\n",
    "    unique_sentences = []\n",
    "    seen = set()\n",
    "\n",
    "    for sentence in sentences:\n",
    "        cleaned_sentence = sentence.strip()\n",
    "        if cleaned_sentence and cleaned_sentence not in seen:\n",
    "            unique_sentences.append(cleaned_sentence)\n",
    "            seen.add(cleaned_sentence)\n",
    "\n",
    "    # 3️⃣ 각 문장을 개행 문자(\"\\n\")로 연결하여 출력\n",
    "    return \"\\n\".join(unique_sentences).strip()\n",
    "\n",
    "\n",
    "# 🔹 생성된 텍스트에서 \"Summary:\" 이후의 부분만 추출\n",
    "if \"Summary:\" in generated_text:\n",
    "    summary_part = generated_text.split(\"Summary:\")[-1].strip()\n",
    "else:\n",
    "    summary_part = generated_text.strip()\n",
    "\n",
    "# 🔹 중복 제거 후 정리된 문장 적용\n",
    "final_summary = refine_sentences(summary_part)\n",
    "\n",
    "print(\"Final Refined Summary:\")\n",
    "print(final_summary)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "# # 8. 생성된 토큰 디코딩 후 출력\n",
    "# generated_text = tokenizer.decode(generated_tokens[0], skip_special_tokens=True)\n",
    "# print(\"생성된 요약:\")\n",
    "# print(generated_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "whisper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
