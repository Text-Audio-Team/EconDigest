{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/home/wanted-1/potenup-workspace/Project/project3/team2/SY/Gemma/processed_text.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wanted-1/miniconda3/envs/whisper/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import re\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import PeftModel, prepare_model_for_kbit_training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:01<00:00,  1.79it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): GemmaForCausalLM(\n",
       "      (model): GemmaModel(\n",
       "        (embed_tokens): Embedding(256000, 2048, padding_idx=0)\n",
       "        (layers): ModuleList(\n",
       "          (0-17): 18 x GemmaDecoderLayer(\n",
       "            (self_attn): GemmaAttention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=256, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=256, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "            )\n",
       "            (mlp): GemmaMLP(\n",
       "              (gate_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=16384, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=16384, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=16384, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=16384, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=16384, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=16384, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): GELUActivation()\n",
       "            )\n",
       "            (input_layernorm): GemmaRMSNorm((2048,), eps=1e-06)\n",
       "            (post_attention_layernorm): GemmaRMSNorm((2048,), eps=1e-06)\n",
       "          )\n",
       "        )\n",
       "        (norm): GemmaRMSNorm((2048,), eps=1e-06)\n",
       "        (rotary_emb): GemmaRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=2048, out_features=256000, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. ì„¤ì •: ë² ì´ìŠ¤ ëª¨ë¸ ë° quantization ì„¤ì •\n",
    "BASE_MODEL = \"google/gemma-2b-it\"\n",
    "\n",
    "#ë™ì¼í•œ í™˜ê²½ì—ì„œ ëª¨ë¸ê³¼ LoRA ì–´ëŒ‘í„°ë¥¼ ì˜¬ë°”ë¥´ê²Œ ë¶ˆëŸ¬ì˜¤ê¸° ìœ„í•´ í•„ìš”\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# 2. ë² ì´ìŠ¤ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸° ë° k-bit í•™ìŠµ ì¤€ë¹„\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map={\"\": torch.cuda.current_device()}\n",
    ")\n",
    "base_model = prepare_model_for_kbit_training(base_model)\n",
    "\n",
    "# 3. ì €ìž¥í•œ LoRA ì–´ëŒ‘í„° ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "model = PeftModel.from_pretrained(base_model, \"path/to/save/lora_adapters_ver1_law\")\n",
    "model.eval()  # í‰ê°€ ëª¨ë“œë¡œ ì „í™˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. í† í¬ë‚˜ì´ì € ì´ˆê¸°í™”\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------\n",
    "# 1. ìƒì„± í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¡°ì •\n",
    "generation_kwargs = {\n",
    "    \"max_length\": 4096,           # ìµœëŒ€ ìƒì„± í† í° ìˆ˜ (ìš”ì•½ ê¸¸ì´ ì œí•œ)\n",
    "    \"min_length\": 512,            # ìµœì†Œ ìƒì„± í† í° ìˆ˜\n",
    "    \"do_sample\": True,           # ìƒ˜í”Œë§ ë°©ì‹ ì‚¬ìš©\n",
    "    \"temperature\": 0.5,          # ë‚®ì€ ê°’ì€ ê²°ì •ë¡ ì , ë†’ì€ ê°’ì€ ë‹¤ì–‘ì„± ì¦ê°€\n",
    "    \"top_k\": 50,                 # ìƒìœ„ kê°œ ë‹¨ì–´ ë‚´ì—ì„œ ìƒ˜í”Œë§\n",
    "    \"top_p\": 0.95,               # ëˆ„ì  í™•ë¥  p ë‚´ ë‹¨ì–´ì—ì„œ ìƒ˜í”Œë§\n",
    "    \"num_return_sequences\": 1   # 3ê°œì˜ í›„ë³´ ìš”ì•½ ìƒì„± (í›„ë³´ ë¹„êµë¥¼ ìœ„í•´)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. ìš”ì•½í•  í…ìŠ¤íŠ¸ íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "txt_file_path = \"/home/wanted-1/potenup-workspace/Project/project3/team2/SY/Gemma/processed_text.txt\"  # íŒŒì¼ ê²½ë¡œë¥¼ ìˆ˜ì •í•˜ì„¸ìš”.\n",
    "with open(txt_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    original_text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "make summary\n",
    "#  Write the summary in complete sentences, ensuring that each sentence ends with a period.\n",
    "# - The summary must be composed of at least 3 sentences and no more than 5 sentences.\n",
    "# - Avoid unnecessary elaboration or repetition; be concise and clear.\n",
    "# - Accurately reflect the core information of the original text.\n",
    "# - \"í–ˆìœ¼ë©°\" ë¼ëŠ” í‘œí˜„ ì¤„ì´ê³ , ë¬¸ìž¥ ë‹¨ìœ„ë¡œ ìž‘ì„±\n",
    "# - Avoid using ','. \n",
    "# - Accurately reflect the core information of the original text.\n",
    "# - Do not use compound sentences with connectors like 'and' (minimize expressions similar to \"ë©°,\"); write each sentence independently.\n",
    "# - Do not use commas.\n",
    "# - Make sure the summary is clear, concise, and written in separate sentences.\n",
    "# - Only output the summary.\n",
    "# - Do not include any notes, disclaimers, or additional comments.\n",
    "\n",
    "# After reading the original text below, please write a summary following these guidelines:\n",
    "# - Summarize in English first\n",
    "# - Make sure the summary is clear, concise, and written in separate sentences.\n",
    "# - The summary must be composed of at least 3 sentences and no more than 5 sentences.\n",
    "\n",
    "# - then translate it into Korean.\n",
    "# - Translate sentence by sentence, keeping each sentence distinct.\n",
    "# - Ensure that each translated sentence remains separate and does not merge.\n",
    "# - Do not write '**Note:**' AND 'Note:'\n",
    "# - Do not include any notes, disclaimers, or additional comments. Only output the summary.\n",
    "\n",
    "# - Avoid commas. Write short and clear sentences instead.\n",
    "# - Provide the summary as bullet points\n",
    "# - Each bullet point should include the main ideas, such as key events, people, dates, and any significant background.\n",
    "# - Include all the main ideas and key details of the original text.\n",
    "# - Do not use ','\n",
    "\n",
    "#  Write the summary in complete sentences, ensuring that each sentence ends with a period.\n",
    "# - The summary must be composed of at least 3 sentences and no more than 5 sentences.\n",
    "# - Avoid unnecessary elaboration or repetition; be concise and clear.\n",
    "# - Accurately reflect the core information of the original text.\n",
    "# - \"í–ˆìœ¼ë©°\" ë¼ëŠ” í‘œí˜„ ì¤„ì´ê³ , ë¬¸ìž¥ ë‹¨ìœ„ë¡œ ìž‘ì„±\n",
    "# - Avoid using ','. \n",
    "# - Accurately reflect the core information of the original text.\n",
    "# - Do not use compound sentences with connectors like 'and' (minimize expressions similar to \"ë©°,\"); write each sentence independently.\n",
    "# - Do not use commas.\n",
    "# - Make sure the summary is clear, concise, and written in separate sentences.\n",
    "# - Only output the summary.\n",
    "# - Do not include any notes, disclaimers, or additional comments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. ìƒì„¸ í”„ë¡¬í”„íŠ¸ êµ¬ì„±\n",
    "# í”„ë¡¬í”„íŠ¸ì— êµ¬ì²´ì ì¸ ìš”ì•½ ìž‘ì„± ì§€ì‹œì™€ ì˜ˆì‹œë¥¼ í¬í•¨í•˜ì—¬ ëª¨ë¸ì´ ì–´ë–¤ ì •ë³´ë¥¼ í¬í•¨í•´ì•¼ í•˜ëŠ”ì§€ ëª…í™•ížˆ ìœ ë„í•©ë‹ˆë‹¤.\n",
    "detailed_prompt = \"\"\"\n",
    "\n",
    "After reading the original text below, please perform the following steps in order:\n",
    "\n",
    "1. Translate the original text into English.\n",
    "2. Summarize the translated text by extracting only the most important core points. Present each key point as a numbered item (for example, \"1.\", \"2.\", \"3.\", etc.). Write each point concisely and clearly.\n",
    "3. Finally, translate the numbered summary into Korean.\n",
    "\n",
    "Important:\n",
    "- The final output must be only the summary in Korean.\n",
    "- Use simple and direct language.\n",
    "- Do not include any extra commentary, notes, or repeated information.\n",
    "- Each numbered point must be on a separate line and end with a period.\n",
    "- Do not include any bullet symbols (such as \"*\" or \"-\").\n",
    "- Do not write '**Note:**' AND 'Note:'\n",
    "- Do not include any notes, disclaimers, or additional comments. Only output the summary.\n",
    "\n",
    "Original Text:\n",
    "{original_text}\n",
    "\n",
    "Summary:\n",
    "\"\"\"\n",
    "\n",
    "# ì‹¤ì œ ì›ë¬¸ í…ìŠ¤íŠ¸ (ì˜ˆì‹œë¡œ ì§ì ‘ ìž…ë ¥í•˜ê±°ë‚˜ íŒŒì¼ì—ì„œ ì½ì–´ì˜¬ ìˆ˜ ìžˆìŒ)\n",
    "#original_text = \"ì—¬ê¸°ì— ì‹¤ì œ ì›ë¬¸ í…ìŠ¤íŠ¸ë¥¼ ìž…ë ¥í•©ë‹ˆë‹¤. ì´ í…ìŠ¤íŠ¸ëŠ” ì˜ˆì‹œë¡œì„œ ëª¨ë¸ì´ ìš”ì•½í•  ë‚´ìš©ìž…ë‹ˆë‹¤.\"\n",
    "\n",
    "# í”„ë¡¬í”„íŠ¸ êµ¬ì„±\n",
    "prompt = detailed_prompt.format(original_text=original_text)\n",
    "\n",
    "# 4. ìž…ë ¥ í† í°í™”\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. ëª¨ë¸ì„ í†µí•œ ìš”ì•½ ìƒì„±\n",
    "with torch.no_grad():\n",
    "    output_tokens = model.generate(**inputs, **generation_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------\n",
    "# 4. í›„ì²˜ë¦¬ í•¨ìˆ˜ ì •ì˜: ìƒì„±ëœ í…ìŠ¤íŠ¸ë¥¼ ì •ì œí•©ë‹ˆë‹¤.\n",
    "def post_process(text):\n",
    "    # ì•žë’¤ ê³µë°± ì œê±°\n",
    "    text = text.strip()\n",
    "    # ë¶ˆí•„ìš”í•œ ê³µë°±, ì¤„ë°”ê¿ˆ, ì¤‘ë³µ ë¬¸ìž¥ ë¶€í˜¸ ì œê±° (ì˜ˆì‹œ: ëŠë‚Œí‘œ, ê³µë°± ë“±)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'!{2,}', '!', text)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Refined Summary:\n",
      "êµ­ì •ì›ì´ ë¶í•œ í•´ì»¤ì˜ ìœ„ìž¥ ì·¨ì—…ì„ ì¡°ì‹¬í•˜ë¼ ì£¼ì˜ë ¹ì„ ë‚´ë ¸ëŠ”ë°, ì´ëŠ” êµ­ì •ì›ì´ êµ­ë‚´ í•œ ì—ë„ˆì§€ ê¸°ì—…ì˜ í•´ì™¸ì§€ì‚¬ì— ì·¨ì—…í•˜ë ¤ë˜ ë¶í•œ í•´ì»¤ë¥¼ ì ë°œí•´ ì¡°ì‚¬ ì¤‘ì´ë¼ê³  ë°í˜”ìœ¼ë©°, ì´ í•´ì»¤ëŠ” êµ³ì´ í”Œëž«í¼ ì‚¬ì´íŠ¸ì— ë“±ë¡í•˜ëŠ” ëŒ€ë‹´í•¨ë„ ë³´ì˜€ìœ¼ë©°, êµ­ì •ì›ì€ ëŒ€ë‚¨ ì‚¬ì´ë²„ í…ŒëŸ¬ë¥¼ ì£¼ë„í–ˆë˜ ê¹€ì˜ì² ì˜ ë³µê¸°ë„ ì£¼ì‹œí•˜ê³  ìžˆìœ¼ë©°, ì±„ë„A ë‰´ìŠ¤ ë°°ë‘ì›ì´ë¼ê³  í•œë‹¤.\n",
      "Note:\n",
      "êµ­ì •ì›ì€ ìš°ë¦¬ë‚˜ë¼ê°€ ì˜ì œë¥¼ ì„ í˜¸í•˜ë“¯ ë¶í•œì€ IT ë¶„ì•¼ ì§„ì¶œì„ ì„ í˜¸í•œë‹¤ê³ ë„ í–ˆìŠµë‹ˆë‹¤.\n",
      "êµ­ì •ì›ì€ ëŒ€ë‚¨ ì‚¬ì´ë²„ í…ŒëŸ¬ë¥¼ ì£¼ë„í–ˆë˜ ê¹€ì˜ì² ì˜ ë³µê¸°ë„ ì£¼ì‹œí•˜ê³  ìžˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "generated_text = tokenizer.decode(output_tokens[0], skip_special_tokens=True)\n",
    "# \"ìš”ì•½:\" ì´í›„ì˜ í…ìŠ¤íŠ¸ê°€ ì‹¤ì œ ìš”ì•½ì´ë¼ê³  ê°€ì •\n",
    "import re\n",
    "\n",
    "def refine_sentences(text):\n",
    "    \"\"\"\n",
    "    1. ë¬¸ìž¥ì„ ì ì ˆížˆ ë¶„ë¦¬í•˜ê³  ì¤‘ë³µ ì œê±°\n",
    "    2. ìž˜ëª» ì¶”ê°€ëœ ë§ˆì¹¨í‘œë¥¼ ì •ë¦¬í•˜ì—¬ ìžì—°ìŠ¤ëŸ½ê²Œ ë³€í™˜\n",
    "    \"\"\"\n",
    "    # 1ï¸âƒ£ ë¬¸ìž¥ ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ê¸° (ë§ˆì¹¨í‘œ, ë¬¼ìŒí‘œ, ëŠë‚Œí‘œ ê¸°ì¤€)\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', text.strip())\n",
    "\n",
    "    # 2ï¸âƒ£ ì¤‘ë³µ ë¬¸ìž¥ ì œê±°\n",
    "    unique_sentences = []\n",
    "    seen = set()\n",
    "\n",
    "    for sentence in sentences:\n",
    "        cleaned_sentence = sentence.strip()\n",
    "        if cleaned_sentence and cleaned_sentence not in seen:\n",
    "            unique_sentences.append(cleaned_sentence)\n",
    "            seen.add(cleaned_sentence)\n",
    "\n",
    "    # 3ï¸âƒ£ ê° ë¬¸ìž¥ì„ ê°œí–‰ ë¬¸ìž(\"\\n\")ë¡œ ì—°ê²°í•˜ì—¬ ì¶œë ¥\n",
    "    return \"\\n\".join(unique_sentences).strip()\n",
    "\n",
    "\n",
    "# ðŸ”¹ ìƒì„±ëœ í…ìŠ¤íŠ¸ì—ì„œ \"Summary:\" ì´í›„ì˜ ë¶€ë¶„ë§Œ ì¶”ì¶œ\n",
    "if \"Summary:\" in generated_text:\n",
    "    summary_part = generated_text.split(\"Summary:\")[-1].strip()\n",
    "else:\n",
    "    summary_part = generated_text.strip()\n",
    "\n",
    "# ðŸ”¹ ì¤‘ë³µ ì œê±° í›„ ì •ë¦¬ëœ ë¬¸ìž¥ ì ìš©\n",
    "final_summary = refine_sentences(summary_part)\n",
    "\n",
    "print(\"Final Refined Summary:\")\n",
    "print(final_summary)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "# # 8. ìƒì„±ëœ í† í° ë””ì½”ë”© í›„ ì¶œë ¥\n",
    "# generated_text = tokenizer.decode(generated_tokens[0], skip_special_tokens=True)\n",
    "# print(\"ìƒì„±ëœ ìš”ì•½:\")\n",
    "# print(generated_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "whisper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
