{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wanted-1/miniconda3/envs/whisper/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    default_data_collator\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import zipfile\n",
    "import json\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, Features, Value\n",
    "\n",
    "def preprocess_json(json_path):\n",
    "    \"\"\"\n",
    "    JSON 파일을 로드하여 전처리합니다.\n",
    "    \n",
    "    - 상위에 \"documents\" 키가 있을 경우, 각 문서에서 \"text\" 필드를 \n",
    "      평탄화하여 하나의 문자열로 합치고, \"abstractive\" 필드를 요약으로 사용합니다.\n",
    "    - 최종적으로 \"documents\"와 \"summary\" 두 개의 컬럼을 갖는 DataFrame을 생성합니다.\n",
    "    - 결측값을 제거한 후, 전처리된 데이터를 JSON 파일에 다시 저장합니다.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        # JSON 구조가 딕셔너리이고 \"documents\" 키가 있는 경우 처리\n",
    "        if isinstance(data, dict) and \"documents\" in data:\n",
    "            docs = data[\"documents\"]  # 각 문서가 담긴 리스트\n",
    "            new_data = []\n",
    "            for doc in docs:\n",
    "                # 1. 문서 텍스트 추출: \"text\" 필드는 여러 그룹(리스트)로 구성되어 있음\n",
    "                doc_sentences = []\n",
    "                if \"text\" in doc and isinstance(doc[\"text\"], list):\n",
    "                    for group in doc[\"text\"]:\n",
    "                        # group은 리스트인데, 각 원소는 문장(dict)입니다.\n",
    "                        for sentence_item in group:\n",
    "                            sentence = sentence_item.get(\"sentence\", \"\")\n",
    "                            doc_sentences.append(sentence)\n",
    "                    doc_text = \" \".join(doc_sentences).strip()\n",
    "                else:\n",
    "                    doc_text = \"\"\n",
    "                \n",
    "                # 2. 요약 추출: \"abstractive\" 필드가 있으면 사용 (리스트인 경우 모두 합침)\n",
    "                if \"abstractive\" in doc and doc[\"abstractive\"]:\n",
    "                    if isinstance(doc[\"abstractive\"], list):\n",
    "                        summary_text = \" \".join(doc[\"abstractive\"]).strip()\n",
    "                    else:\n",
    "                        summary_text = str(doc[\"abstractive\"])\n",
    "                else:\n",
    "                    summary_text = \"\"\n",
    "                \n",
    "                new_data.append({\n",
    "                    \"documents\": doc_text,\n",
    "                    \"summary\": summary_text\n",
    "                })\n",
    "            df = pd.DataFrame(new_data)\n",
    "        # 만약 상위에 \"data\" 키가 있다면\n",
    "        elif isinstance(data, dict) and \"data\" in data:\n",
    "            df = pd.DataFrame(data[\"data\"])\n",
    "        elif isinstance(data, list):\n",
    "            df = pd.DataFrame(data)\n",
    "        else:\n",
    "            df = pd.DataFrame(data)\n",
    "        \n",
    "        # 확인: \"documents\"와 \"summary\" 컬럼이 반드시 존재해야 함\n",
    "        if \"documents\" not in df.columns:\n",
    "            raise ValueError(f\"'documents' 필드가 존재하지 않습니다: {json_path}\")\n",
    "        if \"summary\" not in df.columns:\n",
    "            raise ValueError(f\"'summary' 필드가 존재하지 않습니다: {json_path}\")\n",
    "        \n",
    "        # 두 컬럼을 문자열로 변환\n",
    "        df[\"documents\"] = df[\"documents\"].astype(str)\n",
    "        df[\"summary\"] = df[\"summary\"].astype(str)\n",
    "        \n",
    "        # 결측값 제거\n",
    "        df.dropna(inplace=True)\n",
    "        \n",
    "        # 전처리된 데이터를 다시 JSON 파일로 저장 (records 형식)\n",
    "        df.to_json(json_path, orient=\"records\", force_ascii=False)\n",
    "    \n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"⚠️ JSON 파일 {json_path} 전처리 중 오류 발생: {e}\")\n",
    "\n",
    "def load_and_build_dataset_from_zip(zip_path, extracted_dir):\n",
    "    \"\"\"\n",
    "    zip 파일 내의 JSON 파일들을 동적으로 검색하여 데이터셋을 구성합니다.\n",
    "    \n",
    "    1. 지정된 경로에 압축 파일을 해제합니다.\n",
    "    2. glob 모듈을 사용해 모든 JSON 파일을 검색합니다.\n",
    "    3. 정규표현식을 활용하여 파일명에 'train' (대소문자 구분 없이)이 포함된 파일만 필터링합니다.\n",
    "    4. 필터링된 파일들에 대해 전처리를 수행한 후, Hugging Face의 load_dataset 함수로 데이터셋을 구성합니다.\n",
    "    \n",
    "    :param zip_path: 압축 파일의 경로\n",
    "    :param extracted_dir: 압축 해제할 디렉토리\n",
    "    :return: 구성된 데이터셋\n",
    "    \"\"\"\n",
    "    # 압축 해제할 디렉토리가 없으면 생성합니다.\n",
    "    if not os.path.exists(extracted_dir):\n",
    "        os.makedirs(extracted_dir)\n",
    "    \n",
    "    # zip 파일을 열어 지정된 디렉토리에 압축 해제\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extracted_dir)\n",
    "    \n",
    "    # extracted_dir 내의 모든 JSON 파일 검색\n",
    "    json_files = glob.glob(os.path.join(extracted_dir, \"*.json\"))\n",
    "    \n",
    "    # 파일명에 'train'이 포함된 파일만 필터링 (대소문자 무시)\n",
    "    train_files = [f for f in json_files if re.search(r\"train\", os.path.basename(f), re.IGNORECASE)]\n",
    "    \n",
    "    if not train_files:\n",
    "        raise ValueError(\"압축 해제된 디렉토리에서 'train'이 포함된 JSON 파일을 찾을 수 없습니다.\")\n",
    "    \n",
    "    # validation 파일도 필요하면 비슷한 방식으로 필터링 가능 (예시)\n",
    "    validation_files = [f for f in json_files if re.search(r\"validation\", os.path.basename(f), re.IGNORECASE)]\n",
    "    \n",
    "    # 각 train/validation 파일에 대해 전처리 적용\n",
    "    for json_path in train_files:\n",
    "        preprocess_json(json_path)\n",
    "    if validation_files:\n",
    "        for json_path in validation_files:\n",
    "            preprocess_json(json_path)\n",
    "    \n",
    "    # 데이터 파일 딕셔너리 생성: load_dataset은 리스트로 전달받은 여러 파일들을 자동으로 병합합니다.\n",
    "    data_files = {\"train\": train_files}\n",
    "    if validation_files:\n",
    "        data_files[\"validation\"] = validation_files\n",
    "    \n",
    "    # features를 명시하여 각 필드를 문자열로 강제 지정 (데이터 타입 문제 해결)\n",
    "    features = Features({\n",
    "        \"documents\": Value(\"string\"),\n",
    "        \"summary\": Value(\"string\"),\n",
    "    })\n",
    "    \n",
    "    # JSON 파일들로부터 데이터셋 구성\n",
    "    dataset = load_dataset(\"json\", data_files=data_files, features=features)\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 243983 examples [00:07, 34214.49 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['documents', 'summary'],\n",
      "        num_rows: 243983\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 사용 예시:\n",
    "zip_path = \"/home/wanted-1/potenup-workspace/Project/project3/team2/학습데이터/06_문서요약 텍스트/Training/신문기사_train_original.zip\"\n",
    "extracted_dir = \"/home/wanted-1/potenup-workspace/Project/project3/team2/학습데이터/06_문서요약 텍스트/Training/extracted\"\n",
    "dataset = load_and_build_dataset_from_zip(zip_path, extracted_dir)\n",
    "print(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리된 데이터에서 원문과 요약을 직접 가져옵니다.\n",
    "all_originals = dataset['train']['documents']\n",
    "all_summaries = dataset['train']['summary']\n",
    "\n",
    "# for idx, (orig, summ) in enumerate(zip(all_originals, all_summaries))\n",
    "    # print(f\"Document {idx+1}\")\n",
    "    # print(\"Original Text:\")\n",
    "    # print(orig)\n",
    "    # print(\"Summary:\")\n",
    "    # print(summ)\n",
    "    # print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 예를 들어, 전체 문서에서 추출한 원문과 요약문 리스트가 있다고 가정합니다.\n",
    "data = {\n",
    "    \"original_text\": all_originals,\n",
    "    \"summary_text\": all_summaries\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# DataFrame 내용 확인\n",
    "# print(df.head())\n",
    "\n",
    "# CSV 파일로 저장하기\n",
    "df.to_csv(\"extracted_documents_신문기사.csv\", index=False, encoding=\"utf-8-sig\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "whisper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
